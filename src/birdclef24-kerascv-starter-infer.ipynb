{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bc8b84",
   "metadata": {
    "papermill": {
     "duration": 0.007968,
     "end_time": "2024-05-20T10:16:19.292748",
     "exception": false,
     "start_time": "2024-05-20T10:16:19.284780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
    "This starter notebook is provided by the Keras team.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20264c15",
   "metadata": {
    "papermill": {
     "duration": 0.007351,
     "end_time": "2024-05-20T10:16:19.307471",
     "exception": false,
     "start_time": "2024-05-20T10:16:19.300120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BirdCLEF 2024 with [KerasCV](https://github.com/keras-team/keras-cv) and [Keras](https://github.com/keras-team/keras)\n",
    "\n",
    "> The objective of this competition is to identify under-studied Indian bird species by their calls.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://i.ibb.co/47F4P9R/birdclef2024.png\">\n",
    "</div>\n",
    "\n",
    "This notebook guides you through the process of inferring a Deep Learning model to recognize bird species by their songs (audio data). As the inference requires running only on the `CPU`, we had to create a separate notebooks for training and inference. You can find the [training notebook here](https://www.kaggle.com/code/awsaf49/birdclef24-kerascv-starter-train). Just as a recap of the training notebook, it uses the EfficientNetV2 backbone from KerasCV on the competition dataset. That notebook also demonstrates how to convert audio data to mel-spectrograms using Keras.\n",
    "\n",
    "<u>Fun fact</u>: Both the training and inference notebooks are backend-agnostic, supporting TensorFlow, PyTorch, and JAX. Utilizing KerasCV and Keras allows us to choose our preferred backend. Explore more details on [Keras](https://keras.io/keras_core/announcement/).\n",
    "\n",
    "In this notebook, you will learn:\n",
    "\n",
    "- Designing a data pipeline for audio data, including audio-to-spectrogram conversion.\n",
    "- Loading the data efficiently using [`tf.data`](https://www.tensorflow.org/guide/data).\n",
    "- Creating the model using KerasCV presets.\n",
    "- Inferring the trained model.\n",
    "\n",
    "**Note**: For a more in-depth understanding of KerasCV, refer to the [KerasCV guides](https://keras.io/guides/keras_cv/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cf732",
   "metadata": {
    "papermill": {
     "duration": 0.006902,
     "end_time": "2024-05-20T10:16:19.322302",
     "exception": false,
     "start_time": "2024-05-20T10:16:19.315400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries ğŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598e1c0a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:19.338862Z",
     "iopub.status.busy": "2024-05-20T10:16:19.338441Z",
     "iopub.status.idle": "2024-05-20T10:16:44.205777Z",
     "shell.execute_reply": "2024-05-20T10:16:44.204801Z"
    },
    "papermill": {
     "duration": 24.878608,
     "end_time": "2024-05-20T10:16:44.208145",
     "exception": false,
     "start_time": "2024-05-20T10:16:19.329537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 10:59:23.023162: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 10:59:23.149059: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-24 10:59:23.633310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 10:59:24.544335: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/tmp/ipykernel_6067/3978514859.py:24: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = mpl.cm.get_cmap(\"coolwarm\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\" or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import librosa.display as lid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "cmap = mpl.cm.get_cmap(\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec120df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit keras by one thread\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d1a93",
   "metadata": {
    "papermill": {
     "duration": 0.007265,
     "end_time": "2024-05-20T10:16:44.223062",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.215797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration âš™ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad3886f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:44.240696Z",
     "iopub.status.busy": "2024-05-20T10:16:44.240038Z",
     "iopub.status.idle": "2024-05-20T10:16:44.265471Z",
     "shell.execute_reply": "2024-05-20T10:16:44.264370Z"
    },
    "papermill": {
     "duration": 0.037741,
     "end_time": "2024-05-20T10:16:44.268249",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.230508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "\n",
    "    # Input image size and batch size\n",
    "    img_size = [128, 384]\n",
    "\n",
    "    # Audio duration, sample rate, and length\n",
    "    duration = 15  # second\n",
    "    sample_rate = 32000\n",
    "    audio_len = duration * sample_rate\n",
    "\n",
    "    # STFT parameters\n",
    "    nfft = 2028\n",
    "    window = 2048\n",
    "    hop_length = audio_len // (img_size[1] - 1)\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "\n",
    "    # Number of epochs, model name\n",
    "    preset = \"efficientnetv2_b2_imagenet\"\n",
    "\n",
    "    # Class Labels for BirdCLEF 24\n",
    "    class_names = sorted(os.listdir(\"../data/birdclef-2024/train_audio\"))\n",
    "    num_classes = len(class_names)\n",
    "    class_labels = list(range(num_classes))\n",
    "    label2name = dict(zip(class_labels, class_names))\n",
    "    name2label = {v: k for k, v in label2name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e59235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.img_size[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a187a00",
   "metadata": {
    "papermill": {
     "duration": 0.007288,
     "end_time": "2024-05-20T10:16:44.283790",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.276502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reproducibility â™»ï¸\n",
    "Sets value for random seed to produce similar result in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a00b34",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:44.301082Z",
     "iopub.status.busy": "2024-05-20T10:16:44.300659Z",
     "iopub.status.idle": "2024-05-20T10:16:44.306323Z",
     "shell.execute_reply": "2024-05-20T10:16:44.305100Z"
    },
    "papermill": {
     "duration": 0.017412,
     "end_time": "2024-05-20T10:16:44.309115",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.291703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266c1de",
   "metadata": {
    "papermill": {
     "duration": 0.007333,
     "end_time": "2024-05-20T10:16:44.323972",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.316639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Path ğŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ef7728",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:44.343328Z",
     "iopub.status.busy": "2024-05-20T10:16:44.342660Z",
     "iopub.status.idle": "2024-05-20T10:16:44.346631Z",
     "shell.execute_reply": "2024-05-20T10:16:44.345788Z"
    },
    "papermill": {
     "duration": 0.015493,
     "end_time": "2024-05-20T10:16:44.348948",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.333455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data/birdclef-2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bef364",
   "metadata": {
    "papermill": {
     "duration": 0.007282,
     "end_time": "2024-05-20T10:16:44.363806",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.356524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Data ğŸ“–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a1be8c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:44.380461Z",
     "iopub.status.busy": "2024-05-20T10:16:44.380041Z",
     "iopub.status.idle": "2024-05-20T10:16:44.677274Z",
     "shell.execute_reply": "2024-05-20T10:16:44.676071Z"
    },
    "papermill": {
     "duration": 0.309089,
     "end_time": "2024-05-20T10:16:44.680363",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.371274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/birdclef-2024/unlabeled_soundscapes/13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/birdclef-2024/unlabeled_soundscapes/92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/birdclef-2024/unlabeled_soundscapes/13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/birdclef-2024/unlabeled_soundscapes/19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/birdclef-2024/unlabeled_soundscapes/91...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath\n",
       "0  ../data/birdclef-2024/unlabeled_soundscapes/13...\n",
       "1  ../data/birdclef-2024/unlabeled_soundscapes/92...\n",
       "2  ../data/birdclef-2024/unlabeled_soundscapes/13...\n",
       "3  ../data/birdclef-2024/unlabeled_soundscapes/19...\n",
       "4  ../data/birdclef-2024/unlabeled_soundscapes/91..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paths = glob(f\"{BASE_PATH}/test_soundscapes/*ogg\")\n",
    "# During commit use `unlabeled` data as there is no `test` data.\n",
    "# During submission `test` data will automatically be populated.\n",
    "if len(test_paths) == 0:\n",
    "    test_paths = glob(f\"{BASE_PATH}/unlabeled_soundscapes/*ogg\")\n",
    "test_df = pd.DataFrame(test_paths, columns=[\"filepath\"])[0:90]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6180944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84d1ed",
   "metadata": {
    "papermill": {
     "duration": 0.007662,
     "end_time": "2024-05-20T10:16:44.695867",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.688205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling ğŸ¤–\n",
    "\n",
    "Note that our model was trained on `10 second` duration audio files, but we will infer on `5-second` audio files (as per competition rules). To facilitate this, we have set the model input shape to `(None, None, 3)`, which will allow us to have variable-length input during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40233355",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:44.713947Z",
     "iopub.status.busy": "2024-05-20T10:16:44.713554Z",
     "iopub.status.idle": "2024-05-20T10:16:53.448792Z",
     "shell.execute_reply": "2024-05-20T10:16:53.447407Z"
    },
    "papermill": {
     "duration": 8.747866,
     "end_time": "2024-05-20T10:16:53.451919",
     "exception": false,
     "start_time": "2024-05-20T10:16:44.704053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 10:59:26.010305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 10:59:26.010748: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Create an input layer for the model\n",
    "inp = keras.layers.Input(shape=(None, None, 3))\n",
    "# Pretrained backbone\n",
    "backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(\n",
    "    CFG.preset,\n",
    ")\n",
    "out = keras_cv.models.ImageClassifier(\n",
    "    backbone=backbone, num_classes=CFG.num_classes, name=\"classifier\"\n",
    ")(inp)\n",
    "# Build model\n",
    "model = keras.models.Model(inputs=inp, outputs=out)\n",
    "# Load weights of trained model\n",
    "# model.load_weights(\n",
    "#     \"/kaggle/input/birdclef24-kerascv-starter-train/best_model.weights.h5\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa34888",
   "metadata": {
    "papermill": {
     "duration": 0.007854,
     "end_time": "2024-05-20T10:16:53.469454",
     "exception": false,
     "start_time": "2024-05-20T10:16:53.461600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loader ğŸš\n",
    "\n",
    "The following code will decode the raw audio from `.ogg` file and also decode the spectrogram from the `audio` file. Additionally, we will apply Z-Score standardization and Min-Max normalization to ensure consistent inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e79489e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:53.489508Z",
     "iopub.status.busy": "2024-05-20T10:16:53.489040Z",
     "iopub.status.idle": "2024-05-20T10:16:53.505189Z",
     "shell.execute_reply": "2024-05-20T10:16:53.503852Z"
    },
    "papermill": {
     "duration": 0.028642,
     "end_time": "2024-05-20T10:16:53.507889",
     "exception": false,
     "start_time": "2024-05-20T10:16:53.479247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decodes Audio\n",
    "def build_decoder(with_labels=True, dim=1024):\n",
    "    def get_audio(filepath):\n",
    "        file_bytes = tf.io.read_file(filepath)\n",
    "        audio = tfio.audio.decode_vorbis(file_bytes)  # decode .ogg file\n",
    "        audio = tf.cast(audio, tf.float32)\n",
    "        if tf.shape(audio)[1] > 1:  # stereo -> mono\n",
    "            audio = audio[..., 0:1]\n",
    "        audio = tf.squeeze(audio, axis=-1)\n",
    "        return audio\n",
    "\n",
    "    def create_frames(audio, duration=5, sr=32000):\n",
    "        frame_size = int(duration * sr)\n",
    "        audio = tf.pad(\n",
    "            audio[..., None], [[0, tf.shape(audio)[0] % frame_size], [0, 0]]\n",
    "        )  # pad the end\n",
    "        audio = tf.squeeze(audio)  # remove extra dimension added for padding\n",
    "        frames = tf.reshape(audio, [-1, frame_size])  # shape: [num_frames, frame_size]\n",
    "        return frames\n",
    "\n",
    "    def apply_preproc(spec):\n",
    "        # Standardize\n",
    "        mean = tf.math.reduce_mean(spec)\n",
    "        std = tf.math.reduce_std(spec)\n",
    "        spec = tf.where(tf.math.equal(std, 0), spec - mean, (spec - mean) / std)\n",
    "\n",
    "        # Normalize using Min-Max\n",
    "        min_val = tf.math.reduce_min(spec)\n",
    "        max_val = tf.math.reduce_max(spec)\n",
    "        spec = tf.where(\n",
    "            tf.math.equal(max_val - min_val, 0),\n",
    "            spec - min_val,\n",
    "            (spec - min_val) / (max_val - min_val),\n",
    "        )\n",
    "        return spec\n",
    "\n",
    "    def decode(path):\n",
    "        # Load audio file\n",
    "        audio = get_audio(path)\n",
    "        # Split audio file into frames with each having 5 seecond duration\n",
    "        audio = create_frames(audio)\n",
    "        # Convert audio to spectrogram\n",
    "        spec = keras.layers.MelSpectrogram(\n",
    "            num_mel_bins=CFG.img_size[0],\n",
    "            fft_length=CFG.nfft,\n",
    "            sequence_stride=CFG.hop_length,\n",
    "            sampling_rate=CFG.sample_rate,\n",
    "        )(audio)\n",
    "        # Apply normalization and standardization\n",
    "        spec = apply_preproc(spec)\n",
    "        # Covnert spectrogram to 3 channel image (for imagenet)\n",
    "        spec = tf.tile(spec[..., None], [1, 1, 1, 3])\n",
    "        return spec\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e88e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:53.525947Z",
     "iopub.status.busy": "2024-05-20T10:16:53.525543Z",
     "iopub.status.idle": "2024-05-20T10:16:53.534126Z",
     "shell.execute_reply": "2024-05-20T10:16:53.532639Z"
    },
    "papermill": {
     "duration": 0.020945,
     "end_time": "2024-05-20T10:16:53.536901",
     "exception": false,
     "start_time": "2024-05-20T10:16:53.515956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build data loader\n",
    "def build_dataset(paths, batch_size=1, decode_fn=None, cache=False):\n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(dim=CFG.audio_len)  # decoder\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = (paths,)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.map(\n",
    "        decode_fn, num_parallel_calls=AUTO\n",
    "    )  # decode audio to spectrograms then create frames\n",
    "    ds = ds.cache() if cache else ds  # cache files\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)  # create batches\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c3a7407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ classifier (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImageClassifier</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">182</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,025,812</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)  â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ classifier (\u001b[38;5;33mImageClassifier\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m182\u001b[0m)            â”‚     \u001b[38;5;34m9,025,812\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,025,812</span> (34.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,025,812\u001b[0m (34.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,943,524</span> (34.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,943,524\u001b[0m (34.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,288</span> (321.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m82,288\u001b[0m (321.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get size of model\n",
    "# model.build((None, *CFG.img_size))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bd195",
   "metadata": {
    "papermill": {
     "duration": 0.008029,
     "end_time": "2024-05-20T10:16:53.553107",
     "exception": false,
     "start_time": "2024-05-20T10:16:53.545078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference ğŸƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26981187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T10:16:53.571627Z",
     "iopub.status.busy": "2024-05-20T10:16:53.570637Z",
     "iopub.status.idle": "2024-05-20T10:17:05.196335Z",
     "shell.execute_reply": "2024-05-20T10:17:05.194040Z"
    },
    "papermill": {
     "duration": 11.637928,
     "end_time": "2024-05-20T10:17:05.199150",
     "exception": false,
     "start_time": "2024-05-20T10:16:53.561222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 81/90 [00:05<00:00, 16.34it/s]2024-05-24 12:00:09.179410: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "test : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:05<00:00, 17.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize empty list to store ids\n",
    "ids = []\n",
    "\n",
    "# Initialize empty array to store predictions\n",
    "preds = np.empty(shape=(0, CFG.num_classes), dtype=\"float32\")\n",
    "\n",
    "# Build test dataset\n",
    "test_paths = test_df.filepath.tolist()\n",
    "test_ds = build_dataset(paths=test_paths, batch_size=1)\n",
    "\n",
    "# Iterate over each audio file in the test dataset\n",
    "for idx, specs in enumerate(tqdm(iter(test_ds), desc=\"test \", total=len(test_df))):\n",
    "    # Extract the filename without the extension\n",
    "    filename = test_paths[idx].split(\"/\")[-1].replace(\".ogg\", \"\")\n",
    "\n",
    "    # Convert to backend-specific tensor while excluding extra dimension\n",
    "    specs = keras.ops.convert_to_tensor(specs[0])\n",
    "\n",
    "    # # tf tensor into pytorch tensor\n",
    "    # specs = specs.numpy()\n",
    "    # specs = torch.from_numpy(specs).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Predict bird species for all frames in a recording using all trained models\n",
    "    frame_preds = model.predict(specs, verbose=0)\n",
    "\n",
    "    # Create a ID for each frame in a recording using the filename and frame number\n",
    "    frame_ids = [f\"{filename}_{(frame_id+1)*5}\" for frame_id in range(len(frame_preds))]\n",
    "\n",
    "    # Concatenate the ids\n",
    "    ids += frame_ids\n",
    "    # Concatenate the predictions\n",
    "    preds = np.concatenate([preds, frame_preds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "354bb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test :  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 88/90 [00:04<00:00, 21.89it/s]2024-05-24 01:00:38.453911: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "test : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:05<00:00, 17.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "model = timm.create_model(\n",
    "    \"tf_efficientnet_b0_ns\",\n",
    "    pretrained=True,\n",
    "    num_classes=182,\n",
    "    global_pool=\"avg\",\n",
    "    in_chans=3,\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "# Initialize empty list to store ids\n",
    "ids = []\n",
    "\n",
    "# Initialize empty array to store predictions\n",
    "preds = np.empty(shape=(0, CFG.num_classes), dtype=\"float32\")\n",
    "\n",
    "# Build test dataset\n",
    "test_paths = test_df.filepath.tolist()\n",
    "test_ds = build_dataset(paths=test_paths, batch_size=1)\n",
    "\n",
    "# Iterate over each audio file in the test dataset\n",
    "for idx, specs in enumerate(tqdm(iter(test_ds), desc=\"test \", total=len(test_df))):\n",
    "    # Extract the filename without the extension\n",
    "    filename = test_paths[idx].split(\"/\")[-1].replace(\".ogg\", \"\")\n",
    "\n",
    "    # Convert to backend-specific tensor while excluding extra dimension\n",
    "    specs = keras.ops.convert_to_tensor(specs[0])\n",
    "\n",
    "    # # tf tensor into pytorch tensor\n",
    "    # specs = specs.numpy()\n",
    "    # specs = torch.from_numpy(specs).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Predict bird species for all frames in a recording using all trained models\n",
    "    frame_preds = model.predict(specs, verbose=0)\n",
    "\n",
    "    # Create a ID for each frame in a recording using the filename and frame number\n",
    "    frame_ids = [f\"{filename}_{(frame_id+1)*5}\" for frame_id in range(len(frame_preds))]\n",
    "\n",
    "    # Concatenate the ids\n",
    "    ids += frame_ids\n",
    "    # Concatenate the predictions\n",
    "    preds = np.concatenate([preds, frame_preds], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f467380",
   "metadata": {
    "papermill": {
     "duration": 0.008583,
     "end_time": "2024-05-20T10:17:05.216737",
     "exception": false,
     "start_time": "2024-05-20T10:17:05.208154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission âœ‰ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c02c9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T10:17:05.237470Z",
     "iopub.status.busy": "2024-05-20T10:17:05.236247Z",
     "iopub.status.idle": "2024-05-20T10:17:05.366300Z",
     "shell.execute_reply": "2024-05-20T10:17:05.365010Z"
    },
    "papermill": {
     "duration": 0.143612,
     "end_time": "2024-05-20T10:17:05.369097",
     "exception": false,
     "start_time": "2024-05-20T10:17:05.225485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>asbfly</th>\n",
       "      <th>ashdro1</th>\n",
       "      <th>ashpri1</th>\n",
       "      <th>ashwoo2</th>\n",
       "      <th>asikoe2</th>\n",
       "      <th>asiope1</th>\n",
       "      <th>aspfly1</th>\n",
       "      <th>aspswi1</th>\n",
       "      <th>barfly1</th>\n",
       "      <th>...</th>\n",
       "      <th>whbwoo2</th>\n",
       "      <th>whcbar1</th>\n",
       "      <th>whiter2</th>\n",
       "      <th>whrmun</th>\n",
       "      <th>whtkin2</th>\n",
       "      <th>woosan</th>\n",
       "      <th>wynlau1</th>\n",
       "      <th>yebbab1</th>\n",
       "      <th>yebbul3</th>\n",
       "      <th>zitcis1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384345978_5</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.006802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.005953</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>0.004008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384345978_10</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>0.005958</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.005214</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.004012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384345978_15</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.007076</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.004005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384345978_20</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.004012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384345978_25</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.006032</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005132</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>0.004007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id    asbfly   ashdro1   ashpri1   ashwoo2   asikoe2   asiope1  \\\n",
       "0   1384345978_5  0.004106  0.005859  0.006027  0.006039  0.006472  0.004889   \n",
       "1  1384345978_10  0.004106  0.005853  0.006028  0.006041  0.006469  0.004892   \n",
       "2  1384345978_15  0.004105  0.005854  0.006026  0.006039  0.006473  0.004891   \n",
       "3  1384345978_20  0.004104  0.005857  0.006026  0.006040  0.006468  0.004889   \n",
       "4  1384345978_25  0.004104  0.005854  0.006032  0.006042  0.006471  0.004886   \n",
       "\n",
       "    aspfly1   aspswi1   barfly1  ...   whbwoo2   whcbar1   whiter2    whrmun  \\\n",
       "0  0.007669  0.006761  0.006802  ...  0.005133  0.005576  0.005953  0.007071   \n",
       "1  0.007661  0.006760  0.006799  ...  0.005134  0.005575  0.005958  0.007072   \n",
       "2  0.007668  0.006758  0.006799  ...  0.005134  0.005572  0.005952  0.007076   \n",
       "3  0.007661  0.006761  0.006794  ...  0.005134  0.005573  0.005957  0.007075   \n",
       "4  0.007663  0.006758  0.006805  ...  0.005132  0.005575  0.005959  0.007071   \n",
       "\n",
       "    whtkin2    woosan   wynlau1   yebbab1   yebbul3   zitcis1  \n",
       "0  0.005214  0.004628  0.005736  0.005262  0.006484  0.004008  \n",
       "1  0.005214  0.004630  0.005738  0.005261  0.006479  0.004012  \n",
       "2  0.005212  0.004630  0.005738  0.005261  0.006479  0.004005  \n",
       "3  0.005208  0.004631  0.005738  0.005260  0.006473  0.004012  \n",
       "4  0.005218  0.004634  0.005740  0.005261  0.006480  0.004007  \n",
       "\n",
       "[5 rows x 183 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit prediction\n",
    "pred_df = pd.DataFrame(ids, columns=[\"row_id\"])\n",
    "pred_df.loc[:, CFG.class_names] = preds\n",
    "pred_df.to_csv(\"submission.csv\", index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded64f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4320"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd9f7d",
   "metadata": {
    "papermill": {
     "duration": 0.008878,
     "end_time": "2024-05-20T10:17:05.387184",
     "exception": false,
     "start_time": "2024-05-20T10:17:05.378306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference âœï¸\n",
    "* [Fake Speech Detection: Conformer [TF]](https://www.kaggle.com/code/awsaf49/fake-speech-detection-conformer-tf/) by @awsaf49\n",
    "* [RANZCR: EfficientNet TPU Training](https://www.kaggle.com/code/xhlulu/ranzcr-efficientnet-tpu-training) by @xhlulu\n",
    "* [Triple Stratified KFold with TFRecords](https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords) by @cdeotte"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "sourceId": 172963793,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 4596,
     "sourceId": 6125,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4598,
     "sourceId": 6127,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 52.242853,
   "end_time": "2024-05-20T10:17:08.270912",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-20T10:16:16.028059",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
