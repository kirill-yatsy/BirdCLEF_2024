{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as nxrun\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpn68b_train_df = pd.read_csv(\"../../production/dpn68b/v1.train.csv\")\n",
    "tf_efficientnet_b0_ns_train_df = pd.read_csv(\n",
    "    \"../../production/tf_efficientnet_b0_ns/v1.train.csv\"\n",
    ")\n",
    "efficientnet_b3_train_df = pd.read_csv(\"../../production/efficientnet_b3/v1.train.csv\")\n",
    "\n",
    "\n",
    "dpn68b_val_df = pd.read_csv(\"../../production/dpn68b/v1.val.csv\")\n",
    "tf_efficientnet_b0_ns_val_df = pd.read_csv(\n",
    "    \"../../production/tf_efficientnet_b0_ns/v1.val.csv\"\n",
    ")\n",
    "efficientnet_b3_val_df = pd.read_csv(\"../../production/efficientnet_b3/v1.val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         litegr\n",
       "1         spodov\n",
       "2         comsan\n",
       "3        eaywag1\n",
       "4         greegr\n",
       "          ...   \n",
       "58696     grnsan\n",
       "58697     revbul\n",
       "58698    grewar3\n",
       "58699    asikoe2\n",
       "58700     rorpar\n",
       "Length: 58701, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arhmax of first row\n",
    "dpn68b_train_df.iloc[:, 1:].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to modify df and  apply softmax to a row\n",
    "def apply_softmax(df):\n",
    "    df.iloc[:, 1:] = df.iloc[:, 1:].apply(\n",
    "        lambda x: np.exp(x) / np.sum(np.exp(x)), axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on each row softmax but exclude y column\n",
    "dpn68b_train_df = apply_softmax(dpn68b_train_df)\n",
    "tf_efficientnet_b0_ns_train_df = apply_softmax(tf_efficientnet_b0_ns_train_df)\n",
    "efficientnet_b3_train_df = apply_softmax(efficientnet_b3_train_df)\n",
    "\n",
    "dpn68b_val_df = apply_softmax(dpn68b_val_df)\n",
    "tf_efficientnet_b0_ns_val_df = apply_softmax(tf_efficientnet_b0_ns_val_df)\n",
    "efficientnet_b3_val_df = apply_softmax(efficientnet_b3_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y column to int\n",
    "dpn68b_train_df[\"y\"] = dpn68b_train_df[\"y\"].astype(int)\n",
    "tf_efficientnet_b0_ns_train_df[\"y\"] = tf_efficientnet_b0_ns_train_df[\"y\"].astype(int)\n",
    "efficientnet_b3_train_df[\"y\"] = efficientnet_b3_train_df[\"y\"].astype(int)\n",
    "dpn68b_val_df[\"y\"] = dpn68b_val_df[\"y\"].astype(int)\n",
    "tf_efficientnet_b0_ns_val_df[\"y\"] = tf_efficientnet_b0_ns_val_df[\"y\"].astype(int)\n",
    "efficientnet_b3_val_df[\"y\"] = efficientnet_b3_val_df[\"y\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack dataframes together wihoout y culumn\n",
    "train_df = pd.concat(\n",
    "    [dpn68b_train_df, tf_efficientnet_b0_ns_train_df, efficientnet_b3_train_df], axis=1\n",
    ")\n",
    "train_df = train_df.drop(columns=[\"y\"])\n",
    "train_y = dpn68b_train_df[\"y\"]\n",
    "\n",
    "val_df = pd.concat(\n",
    "    [dpn68b_val_df, tf_efficientnet_b0_ns_val_df, efficientnet_b3_val_df], axis=1\n",
    ")\n",
    "val_df = val_df.drop(columns=[\"y\"])\n",
    "val_y = dpn68b_val_df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asbfly',\n",
       " 'ashdro1',\n",
       " 'ashpri1',\n",
       " 'ashwoo2',\n",
       " 'asikoe2',\n",
       " 'asiope1',\n",
       " 'aspfly1',\n",
       " 'aspswi1',\n",
       " 'barfly1',\n",
       " 'barswa',\n",
       " 'bcnher',\n",
       " 'bkcbul1',\n",
       " 'bkrfla1',\n",
       " 'bkskit1',\n",
       " 'bkwsti',\n",
       " 'bladro1',\n",
       " 'blaeag1',\n",
       " 'blakit1',\n",
       " 'blhori1',\n",
       " 'blnmon1',\n",
       " 'blrwar1',\n",
       " 'bncwoo3',\n",
       " 'brakit1',\n",
       " 'brasta1',\n",
       " 'brcful1',\n",
       " 'brfowl1',\n",
       " 'brnhao1',\n",
       " 'brnshr',\n",
       " 'brodro1',\n",
       " 'brwjac1',\n",
       " 'brwowl1',\n",
       " 'btbeat1',\n",
       " 'bwfshr1',\n",
       " 'categr',\n",
       " 'chbeat1',\n",
       " 'cohcuc1',\n",
       " 'comfla1',\n",
       " 'comgre',\n",
       " 'comior1',\n",
       " 'comkin1',\n",
       " 'commoo3',\n",
       " 'commyn',\n",
       " 'compea',\n",
       " 'comros',\n",
       " 'comsan',\n",
       " 'comtai1',\n",
       " 'copbar1',\n",
       " 'crbsun2',\n",
       " 'cregos1',\n",
       " 'crfbar1',\n",
       " 'crseag1',\n",
       " 'dafbab1',\n",
       " 'darter2',\n",
       " 'eaywag1',\n",
       " 'emedov2',\n",
       " 'eucdov',\n",
       " 'eurbla2',\n",
       " 'eurcoo',\n",
       " 'forwag1',\n",
       " 'gargan',\n",
       " 'gloibi',\n",
       " 'goflea1',\n",
       " 'graher1',\n",
       " 'grbeat1',\n",
       " 'grecou1',\n",
       " 'greegr',\n",
       " 'grefla1',\n",
       " 'grehor1',\n",
       " 'grejun2',\n",
       " 'grenig1',\n",
       " 'grewar3',\n",
       " 'grnsan',\n",
       " 'grnwar1',\n",
       " 'grtdro1',\n",
       " 'gryfra',\n",
       " 'grynig2',\n",
       " 'grywag',\n",
       " 'gybpri1',\n",
       " 'gyhcaf1',\n",
       " 'heswoo1',\n",
       " 'hoopoe',\n",
       " 'houcro1',\n",
       " 'houspa',\n",
       " 'inbrob1',\n",
       " 'indpit1',\n",
       " 'indrob1',\n",
       " 'indrol2',\n",
       " 'indtit1',\n",
       " 'ingori1',\n",
       " 'inpher1',\n",
       " 'insbab1',\n",
       " 'insowl1',\n",
       " 'integr',\n",
       " 'isbduc1',\n",
       " 'jerbus2',\n",
       " 'junbab2',\n",
       " 'junmyn1',\n",
       " 'junowl1',\n",
       " 'kenplo1',\n",
       " 'kerlau2',\n",
       " 'labcro1',\n",
       " 'laudov1',\n",
       " 'lblwar1',\n",
       " 'lesyel1',\n",
       " 'lewduc1',\n",
       " 'lirplo',\n",
       " 'litegr',\n",
       " 'litgre1',\n",
       " 'litspi1',\n",
       " 'litswi1',\n",
       " 'lobsun2',\n",
       " 'maghor2',\n",
       " 'malpar1',\n",
       " 'maltro1',\n",
       " 'malwoo1',\n",
       " 'marsan',\n",
       " 'mawthr1',\n",
       " 'moipig1',\n",
       " 'nilfly2',\n",
       " 'niwpig1',\n",
       " 'nutman',\n",
       " 'orihob2',\n",
       " 'oripip1',\n",
       " 'pabflo1',\n",
       " 'paisto1',\n",
       " 'piebus1',\n",
       " 'piekin1',\n",
       " 'placuc3',\n",
       " 'plaflo1',\n",
       " 'plapri1',\n",
       " 'plhpar1',\n",
       " 'pomgrp2',\n",
       " 'purher1',\n",
       " 'pursun3',\n",
       " 'pursun4',\n",
       " 'purswa3',\n",
       " 'putbab1',\n",
       " 'redspu1',\n",
       " 'rerswa1',\n",
       " 'revbul',\n",
       " 'rewbul',\n",
       " 'rewlap1',\n",
       " 'rocpig',\n",
       " 'rorpar',\n",
       " 'rossta2',\n",
       " 'rufbab3',\n",
       " 'ruftre2',\n",
       " 'rufwoo2',\n",
       " 'rutfly6',\n",
       " 'sbeowl1',\n",
       " 'scamin3',\n",
       " 'shikra1',\n",
       " 'smamin1',\n",
       " 'sohmyn1',\n",
       " 'spepic1',\n",
       " 'spodov',\n",
       " 'spoowl1',\n",
       " 'sqtbul1',\n",
       " 'stbkin1',\n",
       " 'sttwoo1',\n",
       " 'thbwar1',\n",
       " 'tibfly3',\n",
       " 'tilwar1',\n",
       " 'vefnut1',\n",
       " 'vehpar1',\n",
       " 'wbbfly1',\n",
       " 'wemhar1',\n",
       " 'whbbul2',\n",
       " 'whbsho3',\n",
       " 'whbtre1',\n",
       " 'whbwag1',\n",
       " 'whbwat1',\n",
       " 'whbwoo2',\n",
       " 'whcbar1',\n",
       " 'whiter2',\n",
       " 'whrmun',\n",
       " 'whtkin2',\n",
       " 'woosan',\n",
       " 'wynlau1',\n",
       " 'yebbab1',\n",
       " 'yebbul3',\n",
       " 'zitcis1',\n",
       " 'asbfly',\n",
       " 'ashdro1',\n",
       " 'ashpri1',\n",
       " 'ashwoo2',\n",
       " 'asikoe2',\n",
       " 'asiope1',\n",
       " 'aspfly1',\n",
       " 'aspswi1',\n",
       " 'barfly1',\n",
       " 'barswa',\n",
       " 'bcnher',\n",
       " 'bkcbul1',\n",
       " 'bkrfla1',\n",
       " 'bkskit1',\n",
       " 'bkwsti',\n",
       " 'bladro1',\n",
       " 'blaeag1',\n",
       " 'blakit1',\n",
       " 'blhori1',\n",
       " 'blnmon1',\n",
       " 'blrwar1',\n",
       " 'bncwoo3',\n",
       " 'brakit1',\n",
       " 'brasta1',\n",
       " 'brcful1',\n",
       " 'brfowl1',\n",
       " 'brnhao1',\n",
       " 'brnshr',\n",
       " 'brodro1',\n",
       " 'brwjac1',\n",
       " 'brwowl1',\n",
       " 'btbeat1',\n",
       " 'bwfshr1',\n",
       " 'categr',\n",
       " 'chbeat1',\n",
       " 'cohcuc1',\n",
       " 'comfla1',\n",
       " 'comgre',\n",
       " 'comior1',\n",
       " 'comkin1',\n",
       " 'commoo3',\n",
       " 'commyn',\n",
       " 'compea',\n",
       " 'comros',\n",
       " 'comsan',\n",
       " 'comtai1',\n",
       " 'copbar1',\n",
       " 'crbsun2',\n",
       " 'cregos1',\n",
       " 'crfbar1',\n",
       " 'crseag1',\n",
       " 'dafbab1',\n",
       " 'darter2',\n",
       " 'eaywag1',\n",
       " 'emedov2',\n",
       " 'eucdov',\n",
       " 'eurbla2',\n",
       " 'eurcoo',\n",
       " 'forwag1',\n",
       " 'gargan',\n",
       " 'gloibi',\n",
       " 'goflea1',\n",
       " 'graher1',\n",
       " 'grbeat1',\n",
       " 'grecou1',\n",
       " 'greegr',\n",
       " 'grefla1',\n",
       " 'grehor1',\n",
       " 'grejun2',\n",
       " 'grenig1',\n",
       " 'grewar3',\n",
       " 'grnsan',\n",
       " 'grnwar1',\n",
       " 'grtdro1',\n",
       " 'gryfra',\n",
       " 'grynig2',\n",
       " 'grywag',\n",
       " 'gybpri1',\n",
       " 'gyhcaf1',\n",
       " 'heswoo1',\n",
       " 'hoopoe',\n",
       " 'houcro1',\n",
       " 'houspa',\n",
       " 'inbrob1',\n",
       " 'indpit1',\n",
       " 'indrob1',\n",
       " 'indrol2',\n",
       " 'indtit1',\n",
       " 'ingori1',\n",
       " 'inpher1',\n",
       " 'insbab1',\n",
       " 'insowl1',\n",
       " 'integr',\n",
       " 'isbduc1',\n",
       " 'jerbus2',\n",
       " 'junbab2',\n",
       " 'junmyn1',\n",
       " 'junowl1',\n",
       " 'kenplo1',\n",
       " 'kerlau2',\n",
       " 'labcro1',\n",
       " 'laudov1',\n",
       " 'lblwar1',\n",
       " 'lesyel1',\n",
       " 'lewduc1',\n",
       " 'lirplo',\n",
       " 'litegr',\n",
       " 'litgre1',\n",
       " 'litspi1',\n",
       " 'litswi1',\n",
       " 'lobsun2',\n",
       " 'maghor2',\n",
       " 'malpar1',\n",
       " 'maltro1',\n",
       " 'malwoo1',\n",
       " 'marsan',\n",
       " 'mawthr1',\n",
       " 'moipig1',\n",
       " 'nilfly2',\n",
       " 'niwpig1',\n",
       " 'nutman',\n",
       " 'orihob2',\n",
       " 'oripip1',\n",
       " 'pabflo1',\n",
       " 'paisto1',\n",
       " 'piebus1',\n",
       " 'piekin1',\n",
       " 'placuc3',\n",
       " 'plaflo1',\n",
       " 'plapri1',\n",
       " 'plhpar1',\n",
       " 'pomgrp2',\n",
       " 'purher1',\n",
       " 'pursun3',\n",
       " 'pursun4',\n",
       " 'purswa3',\n",
       " 'putbab1',\n",
       " 'redspu1',\n",
       " 'rerswa1',\n",
       " 'revbul',\n",
       " 'rewbul',\n",
       " 'rewlap1',\n",
       " 'rocpig',\n",
       " 'rorpar',\n",
       " 'rossta2',\n",
       " 'rufbab3',\n",
       " 'ruftre2',\n",
       " 'rufwoo2',\n",
       " 'rutfly6',\n",
       " 'sbeowl1',\n",
       " 'scamin3',\n",
       " 'shikra1',\n",
       " 'smamin1',\n",
       " 'sohmyn1',\n",
       " 'spepic1',\n",
       " 'spodov',\n",
       " 'spoowl1',\n",
       " 'sqtbul1',\n",
       " 'stbkin1',\n",
       " 'sttwoo1',\n",
       " 'thbwar1',\n",
       " 'tibfly3',\n",
       " 'tilwar1',\n",
       " 'vefnut1',\n",
       " 'vehpar1',\n",
       " 'wbbfly1',\n",
       " 'wemhar1',\n",
       " 'whbbul2',\n",
       " 'whbsho3',\n",
       " 'whbtre1',\n",
       " 'whbwag1',\n",
       " 'whbwat1',\n",
       " 'whbwoo2',\n",
       " 'whcbar1',\n",
       " 'whiter2',\n",
       " 'whrmun',\n",
       " 'whtkin2',\n",
       " 'woosan',\n",
       " 'wynlau1',\n",
       " 'yebbab1',\n",
       " 'yebbul3',\n",
       " 'zitcis1',\n",
       " 'asbfly',\n",
       " 'ashdro1',\n",
       " 'ashpri1',\n",
       " 'ashwoo2',\n",
       " 'asikoe2',\n",
       " 'asiope1',\n",
       " 'aspfly1',\n",
       " 'aspswi1',\n",
       " 'barfly1',\n",
       " 'barswa',\n",
       " 'bcnher',\n",
       " 'bkcbul1',\n",
       " 'bkrfla1',\n",
       " 'bkskit1',\n",
       " 'bkwsti',\n",
       " 'bladro1',\n",
       " 'blaeag1',\n",
       " 'blakit1',\n",
       " 'blhori1',\n",
       " 'blnmon1',\n",
       " 'blrwar1',\n",
       " 'bncwoo3',\n",
       " 'brakit1',\n",
       " 'brasta1',\n",
       " 'brcful1',\n",
       " 'brfowl1',\n",
       " 'brnhao1',\n",
       " 'brnshr',\n",
       " 'brodro1',\n",
       " 'brwjac1',\n",
       " 'brwowl1',\n",
       " 'btbeat1',\n",
       " 'bwfshr1',\n",
       " 'categr',\n",
       " 'chbeat1',\n",
       " 'cohcuc1',\n",
       " 'comfla1',\n",
       " 'comgre',\n",
       " 'comior1',\n",
       " 'comkin1',\n",
       " 'commoo3',\n",
       " 'commyn',\n",
       " 'compea',\n",
       " 'comros',\n",
       " 'comsan',\n",
       " 'comtai1',\n",
       " 'copbar1',\n",
       " 'crbsun2',\n",
       " 'cregos1',\n",
       " 'crfbar1',\n",
       " 'crseag1',\n",
       " 'dafbab1',\n",
       " 'darter2',\n",
       " 'eaywag1',\n",
       " 'emedov2',\n",
       " 'eucdov',\n",
       " 'eurbla2',\n",
       " 'eurcoo',\n",
       " 'forwag1',\n",
       " 'gargan',\n",
       " 'gloibi',\n",
       " 'goflea1',\n",
       " 'graher1',\n",
       " 'grbeat1',\n",
       " 'grecou1',\n",
       " 'greegr',\n",
       " 'grefla1',\n",
       " 'grehor1',\n",
       " 'grejun2',\n",
       " 'grenig1',\n",
       " 'grewar3',\n",
       " 'grnsan',\n",
       " 'grnwar1',\n",
       " 'grtdro1',\n",
       " 'gryfra',\n",
       " 'grynig2',\n",
       " 'grywag',\n",
       " 'gybpri1',\n",
       " 'gyhcaf1',\n",
       " 'heswoo1',\n",
       " 'hoopoe',\n",
       " 'houcro1',\n",
       " 'houspa',\n",
       " 'inbrob1',\n",
       " 'indpit1',\n",
       " 'indrob1',\n",
       " 'indrol2',\n",
       " 'indtit1',\n",
       " 'ingori1',\n",
       " 'inpher1',\n",
       " 'insbab1',\n",
       " 'insowl1',\n",
       " 'integr',\n",
       " 'isbduc1',\n",
       " 'jerbus2',\n",
       " 'junbab2',\n",
       " 'junmyn1',\n",
       " 'junowl1',\n",
       " 'kenplo1',\n",
       " 'kerlau2',\n",
       " 'labcro1',\n",
       " 'laudov1',\n",
       " 'lblwar1',\n",
       " 'lesyel1',\n",
       " 'lewduc1',\n",
       " 'lirplo',\n",
       " 'litegr',\n",
       " 'litgre1',\n",
       " 'litspi1',\n",
       " 'litswi1',\n",
       " 'lobsun2',\n",
       " 'maghor2',\n",
       " 'malpar1',\n",
       " 'maltro1',\n",
       " 'malwoo1',\n",
       " 'marsan',\n",
       " 'mawthr1',\n",
       " 'moipig1',\n",
       " 'nilfly2',\n",
       " 'niwpig1',\n",
       " 'nutman',\n",
       " 'orihob2',\n",
       " 'oripip1',\n",
       " 'pabflo1',\n",
       " 'paisto1',\n",
       " 'piebus1',\n",
       " 'piekin1',\n",
       " 'placuc3',\n",
       " 'plaflo1',\n",
       " 'plapri1',\n",
       " 'plhpar1',\n",
       " 'pomgrp2',\n",
       " 'purher1',\n",
       " 'pursun3',\n",
       " 'pursun4',\n",
       " 'purswa3',\n",
       " 'putbab1',\n",
       " 'redspu1',\n",
       " 'rerswa1',\n",
       " 'revbul',\n",
       " 'rewbul',\n",
       " 'rewlap1',\n",
       " 'rocpig',\n",
       " 'rorpar',\n",
       " 'rossta2',\n",
       " 'rufbab3',\n",
       " 'ruftre2',\n",
       " 'rufwoo2',\n",
       " 'rutfly6',\n",
       " 'sbeowl1',\n",
       " 'scamin3',\n",
       " 'shikra1',\n",
       " 'smamin1',\n",
       " 'sohmyn1',\n",
       " 'spepic1',\n",
       " 'spodov',\n",
       " 'spoowl1',\n",
       " 'sqtbul1',\n",
       " 'stbkin1',\n",
       " 'sttwoo1',\n",
       " 'thbwar1',\n",
       " 'tibfly3',\n",
       " 'tilwar1',\n",
       " 'vefnut1',\n",
       " 'vehpar1',\n",
       " 'wbbfly1',\n",
       " 'wemhar1',\n",
       " 'whbbul2',\n",
       " 'whbsho3',\n",
       " 'whbtre1',\n",
       " 'whbwag1',\n",
       " 'whbwat1',\n",
       " 'whbwoo2',\n",
       " 'whcbar1',\n",
       " 'whiter2',\n",
       " 'whrmun',\n",
       " 'whtkin2',\n",
       " 'woosan',\n",
       " 'wynlau1',\n",
       " 'yebbab1',\n",
       " 'yebbul3',\n",
       " 'zitcis1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_train = train_df.to_numpy()\n",
    "y_train = train_y.to_numpy()\n",
    "\n",
    "meta_features_val = val_df.to_numpy()\n",
    "y_val = val_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = CatBoostClassifier(\n",
    "    iterations=1000, learning_rate=0.01, depth=6, verbose=0, task_type=\"GPU\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x73a38b03ba40>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "meta_model.load_model(\"meta_model.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7a56e0c26bd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(meta_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save meta_model\n",
    "meta_model.save_model(\"meta_model.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_predictions = meta_model.predict(meta_features_val)\n",
    "accuracy = accuracy_score(y_val, meta_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6800899427636958"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(meta_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MetaModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rows from  train to tests df\n",
    "all_data_df = pd.concat([train_df, val_df], axis=0)\n",
    "all_data_y = np.concatenate([y_train, y_val])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_data_df.to_numpy(), all_data_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]  # Number of meta-features\n",
    "hidden_size = 128\n",
    "output_size = len(np.unique(all_data_y))  # Number of classes\n",
    "\n",
    "model = MetaModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3000], Loss: 1.0850\n",
      "Epoch [4/3000], Loss: 1.0824\n",
      "Epoch [6/3000], Loss: 1.0855\n",
      "Epoch [8/3000], Loss: 1.0842\n",
      "Epoch [10/3000], Loss: 1.0848\n",
      "Epoch [12/3000], Loss: 1.0882\n",
      "Epoch [14/3000], Loss: 1.0844\n",
      "Epoch [16/3000], Loss: 1.0884\n",
      "Epoch [18/3000], Loss: 1.0854\n",
      "Epoch [20/3000], Loss: 1.0835\n",
      "Epoch [22/3000], Loss: 1.0870\n",
      "Epoch [24/3000], Loss: 1.0790\n",
      "Epoch [26/3000], Loss: 1.0862\n",
      "Epoch [28/3000], Loss: 1.0862\n",
      "Epoch [30/3000], Loss: 1.0857\n",
      "Epoch [32/3000], Loss: 1.0811\n",
      "Epoch [34/3000], Loss: 1.0826\n",
      "Epoch [36/3000], Loss: 1.0828\n",
      "Epoch [38/3000], Loss: 1.0866\n",
      "Epoch [40/3000], Loss: 1.0887\n",
      "Epoch [42/3000], Loss: 1.0858\n",
      "Epoch [44/3000], Loss: 1.0868\n",
      "Epoch [46/3000], Loss: 1.0855\n",
      "Epoch [48/3000], Loss: 1.0800\n",
      "Epoch [50/3000], Loss: 1.0854\n",
      "Epoch [52/3000], Loss: 1.0871\n",
      "Epoch [54/3000], Loss: 1.0876\n",
      "Epoch [56/3000], Loss: 1.0821\n",
      "Epoch [58/3000], Loss: 1.0847\n",
      "Epoch [60/3000], Loss: 1.0833\n",
      "Epoch [62/3000], Loss: 1.0843\n",
      "Epoch [64/3000], Loss: 1.0835\n",
      "Epoch [66/3000], Loss: 1.0877\n",
      "Epoch [68/3000], Loss: 1.0868\n",
      "Epoch [70/3000], Loss: 1.0835\n",
      "Epoch [72/3000], Loss: 1.0861\n",
      "Epoch [74/3000], Loss: 1.0884\n",
      "Epoch [76/3000], Loss: 1.0836\n",
      "Epoch [78/3000], Loss: 1.0834\n",
      "Epoch [80/3000], Loss: 1.0837\n",
      "Epoch [82/3000], Loss: 1.0852\n",
      "Epoch [84/3000], Loss: 1.0885\n",
      "Epoch [86/3000], Loss: 1.0840\n",
      "Epoch [88/3000], Loss: 1.0808\n",
      "Epoch [90/3000], Loss: 1.0865\n",
      "Epoch [92/3000], Loss: 1.0865\n",
      "Epoch [94/3000], Loss: 1.0831\n",
      "Epoch [96/3000], Loss: 1.0803\n",
      "Epoch [98/3000], Loss: 1.0812\n",
      "Epoch [100/3000], Loss: 1.0823\n",
      "Epoch [102/3000], Loss: 1.0844\n",
      "Epoch [104/3000], Loss: 1.0859\n",
      "Epoch [106/3000], Loss: 1.0871\n",
      "Epoch [108/3000], Loss: 1.0839\n",
      "Epoch [110/3000], Loss: 1.0847\n",
      "Epoch [112/3000], Loss: 1.0859\n",
      "Epoch [114/3000], Loss: 1.0886\n",
      "Epoch [116/3000], Loss: 1.0854\n",
      "Epoch [118/3000], Loss: 1.0831\n",
      "Epoch [120/3000], Loss: 1.0789\n",
      "Epoch [122/3000], Loss: 1.0860\n",
      "Epoch [124/3000], Loss: 1.0845\n",
      "Epoch [126/3000], Loss: 1.0827\n",
      "Epoch [128/3000], Loss: 1.0852\n",
      "Epoch [130/3000], Loss: 1.0842\n",
      "Epoch [132/3000], Loss: 1.0834\n",
      "Epoch [134/3000], Loss: 1.0897\n",
      "Epoch [136/3000], Loss: 1.0843\n",
      "Epoch [138/3000], Loss: 1.0849\n",
      "Epoch [140/3000], Loss: 1.0812\n",
      "Epoch [142/3000], Loss: 1.0912\n",
      "Epoch [144/3000], Loss: 1.0810\n",
      "Epoch [146/3000], Loss: 1.0833\n",
      "Epoch [148/3000], Loss: 1.0859\n",
      "Epoch [150/3000], Loss: 1.0817\n",
      "Epoch [152/3000], Loss: 1.0915\n",
      "Epoch [154/3000], Loss: 1.0821\n",
      "Epoch [156/3000], Loss: 1.0791\n",
      "Epoch [158/3000], Loss: 1.0837\n",
      "Epoch [160/3000], Loss: 1.0848\n",
      "Epoch [162/3000], Loss: 1.0831\n",
      "Epoch [164/3000], Loss: 1.0886\n",
      "Epoch [166/3000], Loss: 1.0832\n",
      "Epoch [168/3000], Loss: 1.0845\n",
      "Epoch [170/3000], Loss: 1.0823\n",
      "Epoch [172/3000], Loss: 1.0856\n",
      "Epoch [174/3000], Loss: 1.0826\n",
      "Epoch [176/3000], Loss: 1.0813\n",
      "Epoch [178/3000], Loss: 1.0799\n",
      "Epoch [180/3000], Loss: 1.0789\n",
      "Epoch [182/3000], Loss: 1.0855\n",
      "Epoch [184/3000], Loss: 1.0864\n",
      "Epoch [186/3000], Loss: 1.0842\n",
      "Epoch [188/3000], Loss: 1.0804\n",
      "Epoch [190/3000], Loss: 1.0832\n",
      "Epoch [192/3000], Loss: 1.0836\n",
      "Epoch [194/3000], Loss: 1.0818\n",
      "Epoch [196/3000], Loss: 1.0807\n",
      "Epoch [198/3000], Loss: 1.0776\n",
      "Epoch [200/3000], Loss: 1.0776\n",
      "Epoch [202/3000], Loss: 1.0780\n",
      "Epoch [204/3000], Loss: 1.0776\n",
      "Epoch [206/3000], Loss: 1.0846\n",
      "Epoch [208/3000], Loss: 1.0870\n",
      "Epoch [210/3000], Loss: 1.0856\n",
      "Epoch [212/3000], Loss: 1.0819\n",
      "Epoch [214/3000], Loss: 1.0801\n",
      "Epoch [216/3000], Loss: 1.0809\n",
      "Epoch [218/3000], Loss: 1.0841\n",
      "Epoch [220/3000], Loss: 1.0828\n",
      "Epoch [222/3000], Loss: 1.0880\n",
      "Epoch [224/3000], Loss: 1.0849\n",
      "Epoch [226/3000], Loss: 1.0873\n",
      "Epoch [228/3000], Loss: 1.0841\n",
      "Epoch [230/3000], Loss: 1.0835\n",
      "Epoch [232/3000], Loss: 1.0823\n",
      "Epoch [234/3000], Loss: 1.0817\n",
      "Epoch [236/3000], Loss: 1.0877\n",
      "Epoch [238/3000], Loss: 1.0865\n",
      "Epoch [240/3000], Loss: 1.0831\n",
      "Epoch [242/3000], Loss: 1.0875\n",
      "Epoch [244/3000], Loss: 1.0801\n",
      "Epoch [246/3000], Loss: 1.0872\n",
      "Epoch [248/3000], Loss: 1.0858\n",
      "Epoch [250/3000], Loss: 1.0854\n",
      "Epoch [252/3000], Loss: 1.0766\n",
      "Epoch [254/3000], Loss: 1.0805\n",
      "Epoch [256/3000], Loss: 1.0825\n",
      "Epoch [258/3000], Loss: 1.0841\n",
      "Epoch [260/3000], Loss: 1.0804\n",
      "Epoch [262/3000], Loss: 1.0822\n",
      "Epoch [264/3000], Loss: 1.0840\n",
      "Epoch [266/3000], Loss: 1.0826\n",
      "Epoch [268/3000], Loss: 1.0829\n",
      "Epoch [270/3000], Loss: 1.0827\n",
      "Epoch [272/3000], Loss: 1.0840\n",
      "Epoch [274/3000], Loss: 1.0861\n",
      "Epoch [276/3000], Loss: 1.0877\n",
      "Epoch [278/3000], Loss: 1.0815\n",
      "Epoch [280/3000], Loss: 1.0796\n",
      "Epoch [282/3000], Loss: 1.0770\n",
      "Epoch [284/3000], Loss: 1.0882\n",
      "Epoch [286/3000], Loss: 1.0919\n",
      "Epoch [288/3000], Loss: 1.0844\n",
      "Epoch [290/3000], Loss: 1.0851\n",
      "Epoch [292/3000], Loss: 1.0805\n",
      "Epoch [294/3000], Loss: 1.0802\n",
      "Epoch [296/3000], Loss: 1.0769\n",
      "Epoch [298/3000], Loss: 1.0824\n",
      "Epoch [300/3000], Loss: 1.0815\n",
      "Epoch [302/3000], Loss: 1.0844\n",
      "Epoch [304/3000], Loss: 1.0858\n",
      "Epoch [306/3000], Loss: 1.0856\n",
      "Epoch [308/3000], Loss: 1.0822\n",
      "Epoch [310/3000], Loss: 1.0805\n",
      "Epoch [312/3000], Loss: 1.0843\n",
      "Epoch [314/3000], Loss: 1.0836\n",
      "Epoch [316/3000], Loss: 1.0879\n",
      "Epoch [318/3000], Loss: 1.0822\n",
      "Epoch [320/3000], Loss: 1.0811\n",
      "Epoch [322/3000], Loss: 1.0849\n",
      "Epoch [324/3000], Loss: 1.0813\n",
      "Epoch [326/3000], Loss: 1.0821\n",
      "Epoch [328/3000], Loss: 1.0769\n",
      "Epoch [330/3000], Loss: 1.0881\n",
      "Epoch [332/3000], Loss: 1.0823\n",
      "Epoch [334/3000], Loss: 1.0825\n",
      "Epoch [336/3000], Loss: 1.0852\n",
      "Epoch [338/3000], Loss: 1.0864\n",
      "Epoch [340/3000], Loss: 1.0875\n",
      "Epoch [342/3000], Loss: 1.0850\n",
      "Epoch [344/3000], Loss: 1.0808\n",
      "Epoch [346/3000], Loss: 1.0838\n",
      "Epoch [348/3000], Loss: 1.0868\n",
      "Epoch [350/3000], Loss: 1.0771\n",
      "Epoch [352/3000], Loss: 1.0841\n",
      "Epoch [354/3000], Loss: 1.0809\n",
      "Epoch [356/3000], Loss: 1.0837\n",
      "Epoch [358/3000], Loss: 1.0812\n",
      "Epoch [360/3000], Loss: 1.0801\n",
      "Epoch [362/3000], Loss: 1.0845\n",
      "Epoch [364/3000], Loss: 1.0828\n",
      "Epoch [366/3000], Loss: 1.0854\n",
      "Epoch [368/3000], Loss: 1.0839\n",
      "Epoch [370/3000], Loss: 1.0829\n",
      "Epoch [372/3000], Loss: 1.0801\n",
      "Epoch [374/3000], Loss: 1.0841\n",
      "Epoch [376/3000], Loss: 1.0830\n",
      "Epoch [378/3000], Loss: 1.0821\n",
      "Epoch [380/3000], Loss: 1.0833\n",
      "Epoch [382/3000], Loss: 1.0825\n",
      "Epoch [384/3000], Loss: 1.0860\n",
      "Epoch [386/3000], Loss: 1.0852\n",
      "Epoch [388/3000], Loss: 1.0821\n",
      "Epoch [390/3000], Loss: 1.0876\n",
      "Epoch [392/3000], Loss: 1.0864\n",
      "Epoch [394/3000], Loss: 1.0869\n",
      "Epoch [396/3000], Loss: 1.0829\n",
      "Epoch [398/3000], Loss: 1.0784\n",
      "Epoch [400/3000], Loss: 1.0813\n",
      "Epoch [402/3000], Loss: 1.0814\n",
      "Epoch [404/3000], Loss: 1.0857\n",
      "Epoch [406/3000], Loss: 1.0841\n",
      "Epoch [408/3000], Loss: 1.0854\n",
      "Epoch [410/3000], Loss: 1.0831\n",
      "Epoch [412/3000], Loss: 1.0862\n",
      "Epoch [414/3000], Loss: 1.0803\n",
      "Epoch [416/3000], Loss: 1.0861\n",
      "Epoch [418/3000], Loss: 1.0791\n",
      "Epoch [420/3000], Loss: 1.0824\n",
      "Epoch [422/3000], Loss: 1.0828\n",
      "Epoch [424/3000], Loss: 1.0797\n",
      "Epoch [426/3000], Loss: 1.0829\n",
      "Epoch [428/3000], Loss: 1.0841\n",
      "Epoch [430/3000], Loss: 1.0827\n",
      "Epoch [432/3000], Loss: 1.0873\n",
      "Epoch [434/3000], Loss: 1.0875\n",
      "Epoch [436/3000], Loss: 1.0867\n",
      "Epoch [438/3000], Loss: 1.0881\n",
      "Epoch [440/3000], Loss: 1.0867\n",
      "Epoch [442/3000], Loss: 1.0879\n",
      "Epoch [444/3000], Loss: 1.0848\n",
      "Epoch [446/3000], Loss: 1.0797\n",
      "Epoch [448/3000], Loss: 1.0827\n",
      "Epoch [450/3000], Loss: 1.0791\n",
      "Epoch [452/3000], Loss: 1.0783\n",
      "Epoch [454/3000], Loss: 1.0843\n",
      "Epoch [456/3000], Loss: 1.0834\n",
      "Epoch [458/3000], Loss: 1.0823\n",
      "Epoch [460/3000], Loss: 1.0842\n",
      "Epoch [462/3000], Loss: 1.0841\n",
      "Epoch [464/3000], Loss: 1.0877\n",
      "Epoch [466/3000], Loss: 1.0915\n",
      "Epoch [468/3000], Loss: 1.0796\n",
      "Epoch [470/3000], Loss: 1.0838\n",
      "Epoch [472/3000], Loss: 1.0859\n",
      "Epoch [474/3000], Loss: 1.0816\n",
      "Epoch [476/3000], Loss: 1.0824\n",
      "Epoch [478/3000], Loss: 1.0878\n",
      "Epoch [480/3000], Loss: 1.0841\n",
      "Epoch [482/3000], Loss: 1.0794\n",
      "Epoch [484/3000], Loss: 1.0812\n",
      "Epoch [486/3000], Loss: 1.0803\n",
      "Epoch [488/3000], Loss: 1.0846\n",
      "Epoch [490/3000], Loss: 1.0836\n",
      "Epoch [492/3000], Loss: 1.0848\n",
      "Epoch [494/3000], Loss: 1.0822\n",
      "Epoch [496/3000], Loss: 1.0817\n",
      "Epoch [498/3000], Loss: 1.0790\n",
      "Epoch [500/3000], Loss: 1.0819\n",
      "Epoch [502/3000], Loss: 1.0817\n",
      "Epoch [504/3000], Loss: 1.0807\n",
      "Epoch [506/3000], Loss: 1.0822\n",
      "Epoch [508/3000], Loss: 1.0835\n",
      "Epoch [510/3000], Loss: 1.0835\n",
      "Epoch [512/3000], Loss: 1.0793\n",
      "Epoch [514/3000], Loss: 1.0829\n",
      "Epoch [516/3000], Loss: 1.0798\n",
      "Epoch [518/3000], Loss: 1.0799\n",
      "Epoch [520/3000], Loss: 1.0810\n",
      "Epoch [522/3000], Loss: 1.0863\n",
      "Epoch [524/3000], Loss: 1.0817\n",
      "Epoch [526/3000], Loss: 1.0862\n",
      "Epoch [528/3000], Loss: 1.0847\n",
      "Epoch [530/3000], Loss: 1.0831\n",
      "Epoch [532/3000], Loss: 1.0806\n",
      "Epoch [534/3000], Loss: 1.0798\n",
      "Epoch [536/3000], Loss: 1.0815\n",
      "Epoch [538/3000], Loss: 1.0810\n",
      "Epoch [540/3000], Loss: 1.0793\n",
      "Epoch [542/3000], Loss: 1.0817\n",
      "Epoch [544/3000], Loss: 1.0841\n",
      "Epoch [546/3000], Loss: 1.0811\n",
      "Epoch [548/3000], Loss: 1.0888\n",
      "Epoch [550/3000], Loss: 1.0857\n",
      "Epoch [552/3000], Loss: 1.0886\n",
      "Epoch [554/3000], Loss: 1.0841\n",
      "Epoch [556/3000], Loss: 1.0797\n",
      "Epoch [558/3000], Loss: 1.0839\n",
      "Epoch [560/3000], Loss: 1.0793\n",
      "Epoch [562/3000], Loss: 1.0792\n",
      "Epoch [564/3000], Loss: 1.0889\n",
      "Epoch [566/3000], Loss: 1.0826\n",
      "Epoch [568/3000], Loss: 1.0799\n",
      "Epoch [570/3000], Loss: 1.0814\n",
      "Epoch [572/3000], Loss: 1.0788\n",
      "Epoch [574/3000], Loss: 1.0856\n",
      "Epoch [576/3000], Loss: 1.0815\n",
      "Epoch [578/3000], Loss: 1.0803\n",
      "Epoch [580/3000], Loss: 1.0866\n",
      "Epoch [582/3000], Loss: 1.0832\n",
      "Epoch [584/3000], Loss: 1.0821\n",
      "Epoch [586/3000], Loss: 1.0860\n",
      "Epoch [588/3000], Loss: 1.0805\n",
      "Epoch [590/3000], Loss: 1.0886\n",
      "Epoch [592/3000], Loss: 1.0835\n",
      "Epoch [594/3000], Loss: 1.0797\n",
      "Epoch [596/3000], Loss: 1.0783\n",
      "Epoch [598/3000], Loss: 1.0870\n",
      "Epoch [600/3000], Loss: 1.0818\n",
      "Epoch [602/3000], Loss: 1.0836\n",
      "Epoch [604/3000], Loss: 1.0818\n",
      "Epoch [606/3000], Loss: 1.0804\n",
      "Epoch [608/3000], Loss: 1.0822\n",
      "Epoch [610/3000], Loss: 1.0800\n",
      "Epoch [612/3000], Loss: 1.0853\n",
      "Epoch [614/3000], Loss: 1.0871\n",
      "Epoch [616/3000], Loss: 1.0848\n",
      "Epoch [618/3000], Loss: 1.0835\n",
      "Epoch [620/3000], Loss: 1.0811\n",
      "Epoch [622/3000], Loss: 1.0833\n",
      "Epoch [624/3000], Loss: 1.0790\n",
      "Epoch [626/3000], Loss: 1.0871\n",
      "Epoch [628/3000], Loss: 1.0866\n",
      "Epoch [630/3000], Loss: 1.0887\n",
      "Epoch [632/3000], Loss: 1.0838\n",
      "Epoch [634/3000], Loss: 1.0839\n",
      "Epoch [636/3000], Loss: 1.0821\n",
      "Epoch [638/3000], Loss: 1.0851\n",
      "Epoch [640/3000], Loss: 1.0819\n",
      "Epoch [642/3000], Loss: 1.0859\n",
      "Epoch [644/3000], Loss: 1.0785\n",
      "Epoch [646/3000], Loss: 1.0823\n",
      "Epoch [648/3000], Loss: 1.0779\n",
      "Epoch [650/3000], Loss: 1.0798\n",
      "Epoch [652/3000], Loss: 1.0815\n",
      "Epoch [654/3000], Loss: 1.0777\n",
      "Epoch [656/3000], Loss: 1.0822\n",
      "Epoch [658/3000], Loss: 1.0813\n",
      "Epoch [660/3000], Loss: 1.0864\n",
      "Epoch [662/3000], Loss: 1.0824\n",
      "Epoch [664/3000], Loss: 1.0837\n",
      "Epoch [666/3000], Loss: 1.0799\n",
      "Epoch [668/3000], Loss: 1.0804\n",
      "Epoch [670/3000], Loss: 1.0776\n",
      "Epoch [672/3000], Loss: 1.0821\n",
      "Epoch [674/3000], Loss: 1.0824\n",
      "Epoch [676/3000], Loss: 1.0799\n",
      "Epoch [678/3000], Loss: 1.0854\n",
      "Epoch [680/3000], Loss: 1.0847\n",
      "Epoch [682/3000], Loss: 1.0855\n",
      "Epoch [684/3000], Loss: 1.0800\n",
      "Epoch [686/3000], Loss: 1.0844\n",
      "Epoch [688/3000], Loss: 1.0814\n",
      "Epoch [690/3000], Loss: 1.0832\n",
      "Epoch [692/3000], Loss: 1.0762\n",
      "Epoch [694/3000], Loss: 1.0841\n",
      "Epoch [696/3000], Loss: 1.0821\n",
      "Epoch [698/3000], Loss: 1.0735\n",
      "Epoch [700/3000], Loss: 1.0797\n",
      "Epoch [702/3000], Loss: 1.0869\n",
      "Epoch [704/3000], Loss: 1.0847\n",
      "Epoch [706/3000], Loss: 1.0839\n",
      "Epoch [708/3000], Loss: 1.0850\n",
      "Epoch [710/3000], Loss: 1.0859\n",
      "Epoch [712/3000], Loss: 1.0815\n",
      "Epoch [714/3000], Loss: 1.0808\n",
      "Epoch [716/3000], Loss: 1.0791\n",
      "Epoch [718/3000], Loss: 1.0776\n",
      "Epoch [720/3000], Loss: 1.0840\n",
      "Epoch [722/3000], Loss: 1.0801\n",
      "Epoch [724/3000], Loss: 1.0835\n",
      "Epoch [726/3000], Loss: 1.0735\n",
      "Epoch [728/3000], Loss: 1.0821\n",
      "Epoch [730/3000], Loss: 1.0803\n",
      "Epoch [732/3000], Loss: 1.0845\n",
      "Epoch [734/3000], Loss: 1.0833\n",
      "Epoch [736/3000], Loss: 1.0795\n",
      "Epoch [738/3000], Loss: 1.0822\n",
      "Epoch [740/3000], Loss: 1.0807\n",
      "Epoch [742/3000], Loss: 1.0790\n",
      "Epoch [744/3000], Loss: 1.0768\n",
      "Epoch [746/3000], Loss: 1.0820\n",
      "Epoch [748/3000], Loss: 1.0793\n",
      "Epoch [750/3000], Loss: 1.0828\n",
      "Epoch [752/3000], Loss: 1.0864\n",
      "Epoch [754/3000], Loss: 1.0822\n",
      "Epoch [756/3000], Loss: 1.0800\n",
      "Epoch [758/3000], Loss: 1.0850\n",
      "Epoch [760/3000], Loss: 1.0806\n",
      "Epoch [762/3000], Loss: 1.0879\n",
      "Epoch [764/3000], Loss: 1.0823\n",
      "Epoch [766/3000], Loss: 1.0820\n",
      "Epoch [768/3000], Loss: 1.0792\n",
      "Epoch [770/3000], Loss: 1.0777\n",
      "Epoch [772/3000], Loss: 1.0823\n",
      "Epoch [774/3000], Loss: 1.0836\n",
      "Epoch [776/3000], Loss: 1.0794\n",
      "Epoch [778/3000], Loss: 1.0835\n",
      "Epoch [780/3000], Loss: 1.0840\n",
      "Epoch [782/3000], Loss: 1.0810\n",
      "Epoch [784/3000], Loss: 1.0847\n",
      "Epoch [786/3000], Loss: 1.0771\n",
      "Epoch [788/3000], Loss: 1.0814\n",
      "Epoch [790/3000], Loss: 1.0778\n",
      "Epoch [792/3000], Loss: 1.0798\n",
      "Epoch [794/3000], Loss: 1.0834\n",
      "Epoch [796/3000], Loss: 1.0788\n",
      "Epoch [798/3000], Loss: 1.0816\n",
      "Epoch [800/3000], Loss: 1.0844\n",
      "Epoch [802/3000], Loss: 1.0775\n",
      "Epoch [804/3000], Loss: 1.0847\n",
      "Epoch [806/3000], Loss: 1.0711\n",
      "Epoch [808/3000], Loss: 1.0815\n",
      "Epoch [810/3000], Loss: 1.0819\n",
      "Epoch [812/3000], Loss: 1.0820\n",
      "Epoch [814/3000], Loss: 1.0818\n",
      "Epoch [816/3000], Loss: 1.0800\n",
      "Epoch [818/3000], Loss: 1.0848\n",
      "Epoch [820/3000], Loss: 1.0801\n",
      "Epoch [822/3000], Loss: 1.0803\n",
      "Epoch [824/3000], Loss: 1.0833\n",
      "Epoch [826/3000], Loss: 1.0796\n",
      "Epoch [828/3000], Loss: 1.0794\n",
      "Epoch [830/3000], Loss: 1.0761\n",
      "Epoch [832/3000], Loss: 1.0803\n",
      "Epoch [834/3000], Loss: 1.0801\n",
      "Epoch [836/3000], Loss: 1.0806\n",
      "Epoch [838/3000], Loss: 1.0748\n",
      "Epoch [840/3000], Loss: 1.0781\n",
      "Epoch [842/3000], Loss: 1.0807\n",
      "Epoch [844/3000], Loss: 1.0772\n",
      "Epoch [846/3000], Loss: 1.0816\n",
      "Epoch [848/3000], Loss: 1.0802\n",
      "Epoch [850/3000], Loss: 1.0815\n",
      "Epoch [852/3000], Loss: 1.0795\n",
      "Epoch [854/3000], Loss: 1.0819\n",
      "Epoch [856/3000], Loss: 1.0806\n",
      "Epoch [858/3000], Loss: 1.0819\n",
      "Epoch [860/3000], Loss: 1.0814\n",
      "Epoch [862/3000], Loss: 1.0767\n",
      "Epoch [864/3000], Loss: 1.0799\n",
      "Epoch [866/3000], Loss: 1.0836\n",
      "Epoch [868/3000], Loss: 1.0860\n",
      "Epoch [870/3000], Loss: 1.0801\n",
      "Epoch [872/3000], Loss: 1.0814\n",
      "Epoch [874/3000], Loss: 1.0804\n",
      "Epoch [876/3000], Loss: 1.0828\n",
      "Epoch [878/3000], Loss: 1.0814\n",
      "Epoch [880/3000], Loss: 1.0787\n",
      "Epoch [882/3000], Loss: 1.0813\n",
      "Epoch [884/3000], Loss: 1.0852\n",
      "Epoch [886/3000], Loss: 1.0835\n",
      "Epoch [888/3000], Loss: 1.0791\n",
      "Epoch [890/3000], Loss: 1.0864\n",
      "Epoch [892/3000], Loss: 1.0826\n",
      "Epoch [894/3000], Loss: 1.0805\n",
      "Epoch [896/3000], Loss: 1.0812\n",
      "Epoch [898/3000], Loss: 1.0798\n",
      "Epoch [900/3000], Loss: 1.0810\n",
      "Epoch [902/3000], Loss: 1.0806\n",
      "Epoch [904/3000], Loss: 1.0810\n",
      "Epoch [906/3000], Loss: 1.0782\n",
      "Epoch [908/3000], Loss: 1.0813\n",
      "Epoch [910/3000], Loss: 1.0788\n",
      "Epoch [912/3000], Loss: 1.0835\n",
      "Epoch [914/3000], Loss: 1.0845\n",
      "Epoch [916/3000], Loss: 1.0826\n",
      "Epoch [918/3000], Loss: 1.0790\n",
      "Epoch [920/3000], Loss: 1.0823\n",
      "Epoch [922/3000], Loss: 1.0846\n",
      "Epoch [924/3000], Loss: 1.0857\n",
      "Epoch [926/3000], Loss: 1.0797\n",
      "Epoch [928/3000], Loss: 1.0782\n",
      "Epoch [930/3000], Loss: 1.0792\n",
      "Epoch [932/3000], Loss: 1.0783\n",
      "Epoch [934/3000], Loss: 1.0879\n",
      "Epoch [936/3000], Loss: 1.0806\n",
      "Epoch [938/3000], Loss: 1.0849\n",
      "Epoch [940/3000], Loss: 1.0883\n",
      "Epoch [942/3000], Loss: 1.0822\n",
      "Epoch [944/3000], Loss: 1.0772\n",
      "Epoch [946/3000], Loss: 1.0819\n",
      "Epoch [948/3000], Loss: 1.0801\n",
      "Epoch [950/3000], Loss: 1.0765\n",
      "Epoch [952/3000], Loss: 1.0816\n",
      "Epoch [954/3000], Loss: 1.0789\n",
      "Epoch [956/3000], Loss: 1.0820\n",
      "Epoch [958/3000], Loss: 1.0825\n",
      "Epoch [960/3000], Loss: 1.0817\n",
      "Epoch [962/3000], Loss: 1.0794\n",
      "Epoch [964/3000], Loss: 1.0844\n",
      "Epoch [966/3000], Loss: 1.0859\n",
      "Epoch [968/3000], Loss: 1.0833\n",
      "Epoch [970/3000], Loss: 1.0844\n",
      "Epoch [972/3000], Loss: 1.0837\n",
      "Epoch [974/3000], Loss: 1.0828\n",
      "Epoch [976/3000], Loss: 1.0804\n",
      "Epoch [978/3000], Loss: 1.0822\n",
      "Epoch [980/3000], Loss: 1.0830\n",
      "Epoch [982/3000], Loss: 1.0822\n",
      "Epoch [984/3000], Loss: 1.0798\n",
      "Epoch [986/3000], Loss: 1.0818\n",
      "Epoch [988/3000], Loss: 1.0799\n",
      "Epoch [990/3000], Loss: 1.0845\n",
      "Epoch [992/3000], Loss: 1.0821\n",
      "Epoch [994/3000], Loss: 1.0809\n",
      "Epoch [996/3000], Loss: 1.0800\n",
      "Epoch [998/3000], Loss: 1.0819\n",
      "Epoch [1000/3000], Loss: 1.0774\n",
      "Epoch [1002/3000], Loss: 1.0822\n",
      "Epoch [1004/3000], Loss: 1.0792\n",
      "Epoch [1006/3000], Loss: 1.0732\n",
      "Epoch [1008/3000], Loss: 1.0787\n",
      "Epoch [1010/3000], Loss: 1.0754\n",
      "Epoch [1012/3000], Loss: 1.0797\n",
      "Epoch [1014/3000], Loss: 1.0770\n",
      "Epoch [1016/3000], Loss: 1.0853\n",
      "Epoch [1018/3000], Loss: 1.0784\n",
      "Epoch [1020/3000], Loss: 1.0872\n",
      "Epoch [1022/3000], Loss: 1.0826\n",
      "Epoch [1024/3000], Loss: 1.0819\n",
      "Epoch [1026/3000], Loss: 1.0813\n",
      "Epoch [1028/3000], Loss: 1.0812\n",
      "Epoch [1030/3000], Loss: 1.0764\n",
      "Epoch [1032/3000], Loss: 1.0849\n",
      "Epoch [1034/3000], Loss: 1.0801\n",
      "Epoch [1036/3000], Loss: 1.0799\n",
      "Epoch [1038/3000], Loss: 1.0827\n",
      "Epoch [1040/3000], Loss: 1.0745\n",
      "Epoch [1042/3000], Loss: 1.0798\n",
      "Epoch [1044/3000], Loss: 1.0775\n",
      "Epoch [1046/3000], Loss: 1.0817\n",
      "Epoch [1048/3000], Loss: 1.0862\n",
      "Epoch [1050/3000], Loss: 1.0777\n",
      "Epoch [1052/3000], Loss: 1.0760\n",
      "Epoch [1054/3000], Loss: 1.0759\n",
      "Epoch [1056/3000], Loss: 1.0814\n",
      "Epoch [1058/3000], Loss: 1.0789\n",
      "Epoch [1060/3000], Loss: 1.0769\n",
      "Epoch [1062/3000], Loss: 1.0793\n",
      "Epoch [1064/3000], Loss: 1.0810\n",
      "Epoch [1066/3000], Loss: 1.0827\n",
      "Epoch [1068/3000], Loss: 1.0785\n",
      "Epoch [1070/3000], Loss: 1.0824\n",
      "Epoch [1072/3000], Loss: 1.0786\n",
      "Epoch [1074/3000], Loss: 1.0807\n",
      "Epoch [1076/3000], Loss: 1.0729\n",
      "Epoch [1078/3000], Loss: 1.0844\n",
      "Epoch [1080/3000], Loss: 1.0805\n",
      "Epoch [1082/3000], Loss: 1.0784\n",
      "Epoch [1084/3000], Loss: 1.0761\n",
      "Epoch [1086/3000], Loss: 1.0804\n",
      "Epoch [1088/3000], Loss: 1.0784\n",
      "Epoch [1090/3000], Loss: 1.0850\n",
      "Epoch [1092/3000], Loss: 1.0825\n",
      "Epoch [1094/3000], Loss: 1.0826\n",
      "Epoch [1096/3000], Loss: 1.0795\n",
      "Epoch [1098/3000], Loss: 1.0828\n",
      "Epoch [1100/3000], Loss: 1.0801\n",
      "Epoch [1102/3000], Loss: 1.0850\n",
      "Epoch [1104/3000], Loss: 1.0819\n",
      "Epoch [1106/3000], Loss: 1.0829\n",
      "Epoch [1108/3000], Loss: 1.0782\n",
      "Epoch [1110/3000], Loss: 1.0802\n",
      "Epoch [1112/3000], Loss: 1.0785\n",
      "Epoch [1114/3000], Loss: 1.0753\n",
      "Epoch [1116/3000], Loss: 1.0813\n",
      "Epoch [1118/3000], Loss: 1.0853\n",
      "Epoch [1120/3000], Loss: 1.0802\n",
      "Epoch [1122/3000], Loss: 1.0815\n",
      "Epoch [1124/3000], Loss: 1.0780\n",
      "Epoch [1126/3000], Loss: 1.0787\n",
      "Epoch [1128/3000], Loss: 1.0828\n",
      "Epoch [1130/3000], Loss: 1.0804\n",
      "Epoch [1132/3000], Loss: 1.0820\n",
      "Epoch [1134/3000], Loss: 1.0860\n",
      "Epoch [1136/3000], Loss: 1.0766\n",
      "Epoch [1138/3000], Loss: 1.0828\n",
      "Epoch [1140/3000], Loss: 1.0829\n",
      "Epoch [1142/3000], Loss: 1.0819\n",
      "Epoch [1144/3000], Loss: 1.0810\n",
      "Epoch [1146/3000], Loss: 1.0804\n",
      "Epoch [1148/3000], Loss: 1.0753\n",
      "Epoch [1150/3000], Loss: 1.0770\n",
      "Epoch [1152/3000], Loss: 1.0779\n",
      "Epoch [1154/3000], Loss: 1.0786\n",
      "Epoch [1156/3000], Loss: 1.0846\n",
      "Epoch [1158/3000], Loss: 1.0846\n",
      "Epoch [1160/3000], Loss: 1.0798\n",
      "Epoch [1162/3000], Loss: 1.0796\n",
      "Epoch [1164/3000], Loss: 1.0850\n",
      "Epoch [1166/3000], Loss: 1.0788\n",
      "Epoch [1168/3000], Loss: 1.0811\n",
      "Epoch [1170/3000], Loss: 1.0756\n",
      "Epoch [1172/3000], Loss: 1.0789\n",
      "Epoch [1174/3000], Loss: 1.0804\n",
      "Epoch [1176/3000], Loss: 1.0780\n",
      "Epoch [1178/3000], Loss: 1.0762\n",
      "Epoch [1180/3000], Loss: 1.0810\n",
      "Epoch [1182/3000], Loss: 1.0845\n",
      "Epoch [1184/3000], Loss: 1.0825\n",
      "Epoch [1186/3000], Loss: 1.0809\n",
      "Epoch [1188/3000], Loss: 1.0784\n",
      "Epoch [1190/3000], Loss: 1.0824\n",
      "Epoch [1192/3000], Loss: 1.0793\n",
      "Epoch [1194/3000], Loss: 1.0811\n",
      "Epoch [1196/3000], Loss: 1.0781\n",
      "Epoch [1198/3000], Loss: 1.0761\n",
      "Epoch [1200/3000], Loss: 1.0796\n",
      "Epoch [1202/3000], Loss: 1.0828\n",
      "Epoch [1204/3000], Loss: 1.0764\n",
      "Epoch [1206/3000], Loss: 1.0803\n",
      "Epoch [1208/3000], Loss: 1.0784\n",
      "Epoch [1210/3000], Loss: 1.0826\n",
      "Epoch [1212/3000], Loss: 1.0796\n",
      "Epoch [1214/3000], Loss: 1.0843\n",
      "Epoch [1216/3000], Loss: 1.0759\n",
      "Epoch [1218/3000], Loss: 1.0801\n",
      "Epoch [1220/3000], Loss: 1.0797\n",
      "Epoch [1222/3000], Loss: 1.0776\n",
      "Epoch [1224/3000], Loss: 1.0822\n",
      "Epoch [1226/3000], Loss: 1.0832\n",
      "Epoch [1228/3000], Loss: 1.0869\n",
      "Epoch [1230/3000], Loss: 1.0782\n",
      "Epoch [1232/3000], Loss: 1.0780\n",
      "Epoch [1234/3000], Loss: 1.0833\n",
      "Epoch [1236/3000], Loss: 1.0790\n",
      "Epoch [1238/3000], Loss: 1.0752\n",
      "Epoch [1240/3000], Loss: 1.0811\n",
      "Epoch [1242/3000], Loss: 1.0766\n",
      "Epoch [1244/3000], Loss: 1.0817\n",
      "Epoch [1246/3000], Loss: 1.0811\n",
      "Epoch [1248/3000], Loss: 1.0806\n",
      "Epoch [1250/3000], Loss: 1.0817\n",
      "Epoch [1252/3000], Loss: 1.0773\n",
      "Epoch [1254/3000], Loss: 1.0824\n",
      "Epoch [1256/3000], Loss: 1.0775\n",
      "Epoch [1258/3000], Loss: 1.0806\n",
      "Epoch [1260/3000], Loss: 1.0783\n",
      "Epoch [1262/3000], Loss: 1.0810\n",
      "Epoch [1264/3000], Loss: 1.0806\n",
      "Epoch [1266/3000], Loss: 1.0824\n",
      "Epoch [1268/3000], Loss: 1.0823\n",
      "Epoch [1270/3000], Loss: 1.0819\n",
      "Epoch [1272/3000], Loss: 1.0744\n",
      "Epoch [1274/3000], Loss: 1.0836\n",
      "Epoch [1276/3000], Loss: 1.0787\n",
      "Epoch [1278/3000], Loss: 1.0810\n",
      "Epoch [1280/3000], Loss: 1.0787\n",
      "Epoch [1282/3000], Loss: 1.0781\n",
      "Epoch [1284/3000], Loss: 1.0781\n",
      "Epoch [1286/3000], Loss: 1.0814\n",
      "Epoch [1288/3000], Loss: 1.0812\n",
      "Epoch [1290/3000], Loss: 1.0765\n",
      "Epoch [1292/3000], Loss: 1.0831\n",
      "Epoch [1294/3000], Loss: 1.0789\n",
      "Epoch [1296/3000], Loss: 1.0814\n",
      "Epoch [1298/3000], Loss: 1.0825\n",
      "Epoch [1300/3000], Loss: 1.0790\n",
      "Epoch [1302/3000], Loss: 1.0784\n",
      "Epoch [1304/3000], Loss: 1.0800\n",
      "Epoch [1306/3000], Loss: 1.0818\n",
      "Epoch [1308/3000], Loss: 1.0836\n",
      "Epoch [1310/3000], Loss: 1.0795\n",
      "Epoch [1312/3000], Loss: 1.0757\n",
      "Epoch [1314/3000], Loss: 1.0833\n",
      "Epoch [1316/3000], Loss: 1.0826\n",
      "Epoch [1318/3000], Loss: 1.0774\n",
      "Epoch [1320/3000], Loss: 1.0783\n",
      "Epoch [1322/3000], Loss: 1.0799\n",
      "Epoch [1324/3000], Loss: 1.0811\n",
      "Epoch [1326/3000], Loss: 1.0828\n",
      "Epoch [1328/3000], Loss: 1.0781\n",
      "Epoch [1330/3000], Loss: 1.0798\n",
      "Epoch [1332/3000], Loss: 1.0825\n",
      "Epoch [1334/3000], Loss: 1.0818\n",
      "Epoch [1336/3000], Loss: 1.0814\n",
      "Epoch [1338/3000], Loss: 1.0810\n",
      "Epoch [1340/3000], Loss: 1.0795\n",
      "Epoch [1342/3000], Loss: 1.0757\n",
      "Epoch [1344/3000], Loss: 1.0733\n",
      "Epoch [1346/3000], Loss: 1.0827\n",
      "Epoch [1348/3000], Loss: 1.0802\n",
      "Epoch [1350/3000], Loss: 1.0799\n",
      "Epoch [1352/3000], Loss: 1.0798\n",
      "Epoch [1354/3000], Loss: 1.0833\n",
      "Epoch [1356/3000], Loss: 1.0812\n",
      "Epoch [1358/3000], Loss: 1.0806\n",
      "Epoch [1360/3000], Loss: 1.0806\n",
      "Epoch [1362/3000], Loss: 1.0763\n",
      "Epoch [1364/3000], Loss: 1.0813\n",
      "Epoch [1366/3000], Loss: 1.0830\n",
      "Epoch [1368/3000], Loss: 1.0728\n",
      "Epoch [1370/3000], Loss: 1.0771\n",
      "Epoch [1372/3000], Loss: 1.0777\n",
      "Epoch [1374/3000], Loss: 1.0787\n",
      "Epoch [1376/3000], Loss: 1.0802\n",
      "Epoch [1378/3000], Loss: 1.0798\n",
      "Epoch [1380/3000], Loss: 1.0787\n",
      "Epoch [1382/3000], Loss: 1.0809\n",
      "Epoch [1384/3000], Loss: 1.0766\n",
      "Epoch [1386/3000], Loss: 1.0778\n",
      "Epoch [1388/3000], Loss: 1.0840\n",
      "Epoch [1390/3000], Loss: 1.0810\n",
      "Epoch [1392/3000], Loss: 1.0793\n",
      "Epoch [1394/3000], Loss: 1.0838\n",
      "Epoch [1396/3000], Loss: 1.0756\n",
      "Epoch [1398/3000], Loss: 1.0783\n",
      "Epoch [1400/3000], Loss: 1.0770\n",
      "Epoch [1402/3000], Loss: 1.0830\n",
      "Epoch [1404/3000], Loss: 1.0799\n",
      "Epoch [1406/3000], Loss: 1.0865\n",
      "Epoch [1408/3000], Loss: 1.0852\n",
      "Epoch [1410/3000], Loss: 1.0779\n",
      "Epoch [1412/3000], Loss: 1.0786\n",
      "Epoch [1414/3000], Loss: 1.0798\n",
      "Epoch [1416/3000], Loss: 1.0783\n",
      "Epoch [1418/3000], Loss: 1.0817\n",
      "Epoch [1420/3000], Loss: 1.0804\n",
      "Epoch [1422/3000], Loss: 1.0809\n",
      "Epoch [1424/3000], Loss: 1.0817\n",
      "Epoch [1426/3000], Loss: 1.0743\n",
      "Epoch [1428/3000], Loss: 1.0781\n",
      "Epoch [1430/3000], Loss: 1.0822\n",
      "Epoch [1432/3000], Loss: 1.0785\n",
      "Epoch [1434/3000], Loss: 1.0850\n",
      "Epoch [1436/3000], Loss: 1.0818\n",
      "Epoch [1438/3000], Loss: 1.0769\n",
      "Epoch [1440/3000], Loss: 1.0793\n",
      "Epoch [1442/3000], Loss: 1.0778\n",
      "Epoch [1444/3000], Loss: 1.0806\n",
      "Epoch [1446/3000], Loss: 1.0827\n",
      "Epoch [1448/3000], Loss: 1.0826\n",
      "Epoch [1450/3000], Loss: 1.0777\n",
      "Epoch [1452/3000], Loss: 1.0837\n",
      "Epoch [1454/3000], Loss: 1.0781\n",
      "Epoch [1456/3000], Loss: 1.0832\n",
      "Epoch [1458/3000], Loss: 1.0811\n",
      "Epoch [1460/3000], Loss: 1.0806\n",
      "Epoch [1462/3000], Loss: 1.0772\n",
      "Epoch [1464/3000], Loss: 1.0797\n",
      "Epoch [1466/3000], Loss: 1.0766\n",
      "Epoch [1468/3000], Loss: 1.0840\n",
      "Epoch [1470/3000], Loss: 1.0816\n",
      "Epoch [1472/3000], Loss: 1.0868\n",
      "Epoch [1474/3000], Loss: 1.0797\n",
      "Epoch [1476/3000], Loss: 1.0809\n",
      "Epoch [1478/3000], Loss: 1.0784\n",
      "Epoch [1480/3000], Loss: 1.0778\n",
      "Epoch [1482/3000], Loss: 1.0783\n",
      "Epoch [1484/3000], Loss: 1.0787\n",
      "Epoch [1486/3000], Loss: 1.0774\n",
      "Epoch [1488/3000], Loss: 1.0821\n",
      "Epoch [1490/3000], Loss: 1.0834\n",
      "Epoch [1492/3000], Loss: 1.0761\n",
      "Epoch [1494/3000], Loss: 1.0795\n",
      "Epoch [1496/3000], Loss: 1.0749\n",
      "Epoch [1498/3000], Loss: 1.0805\n",
      "Epoch [1500/3000], Loss: 1.0806\n",
      "Epoch [1502/3000], Loss: 1.0831\n",
      "Epoch [1504/3000], Loss: 1.0799\n",
      "Epoch [1506/3000], Loss: 1.0791\n",
      "Epoch [1508/3000], Loss: 1.0820\n",
      "Epoch [1510/3000], Loss: 1.0794\n",
      "Epoch [1512/3000], Loss: 1.0765\n",
      "Epoch [1514/3000], Loss: 1.0771\n",
      "Epoch [1516/3000], Loss: 1.0760\n",
      "Epoch [1518/3000], Loss: 1.0850\n",
      "Epoch [1520/3000], Loss: 1.0765\n",
      "Epoch [1522/3000], Loss: 1.0762\n",
      "Epoch [1524/3000], Loss: 1.0746\n",
      "Epoch [1526/3000], Loss: 1.0809\n",
      "Epoch [1528/3000], Loss: 1.0769\n",
      "Epoch [1530/3000], Loss: 1.0815\n",
      "Epoch [1532/3000], Loss: 1.0784\n",
      "Epoch [1534/3000], Loss: 1.0768\n",
      "Epoch [1536/3000], Loss: 1.0790\n",
      "Epoch [1538/3000], Loss: 1.0825\n",
      "Epoch [1540/3000], Loss: 1.0794\n",
      "Epoch [1542/3000], Loss: 1.0793\n",
      "Epoch [1544/3000], Loss: 1.0722\n",
      "Epoch [1546/3000], Loss: 1.0812\n",
      "Epoch [1548/3000], Loss: 1.0785\n",
      "Epoch [1550/3000], Loss: 1.0782\n",
      "Epoch [1552/3000], Loss: 1.0803\n",
      "Epoch [1554/3000], Loss: 1.0769\n",
      "Epoch [1556/3000], Loss: 1.0800\n",
      "Epoch [1558/3000], Loss: 1.0750\n",
      "Epoch [1560/3000], Loss: 1.0796\n",
      "Epoch [1562/3000], Loss: 1.0809\n",
      "Epoch [1564/3000], Loss: 1.0750\n",
      "Epoch [1566/3000], Loss: 1.0761\n",
      "Epoch [1568/3000], Loss: 1.0773\n",
      "Epoch [1570/3000], Loss: 1.0786\n",
      "Epoch [1572/3000], Loss: 1.0792\n",
      "Epoch [1574/3000], Loss: 1.0793\n",
      "Epoch [1576/3000], Loss: 1.0797\n",
      "Epoch [1578/3000], Loss: 1.0851\n",
      "Epoch [1580/3000], Loss: 1.0843\n",
      "Epoch [1582/3000], Loss: 1.0814\n",
      "Epoch [1584/3000], Loss: 1.0797\n",
      "Epoch [1586/3000], Loss: 1.0820\n",
      "Epoch [1588/3000], Loss: 1.0772\n",
      "Epoch [1590/3000], Loss: 1.0803\n",
      "Epoch [1592/3000], Loss: 1.0810\n",
      "Epoch [1594/3000], Loss: 1.0873\n",
      "Epoch [1596/3000], Loss: 1.0782\n",
      "Epoch [1598/3000], Loss: 1.0811\n",
      "Epoch [1600/3000], Loss: 1.0782\n",
      "Epoch [1602/3000], Loss: 1.0805\n",
      "Epoch [1604/3000], Loss: 1.0737\n",
      "Epoch [1606/3000], Loss: 1.0779\n",
      "Epoch [1608/3000], Loss: 1.0777\n",
      "Epoch [1610/3000], Loss: 1.0825\n",
      "Epoch [1612/3000], Loss: 1.0835\n",
      "Epoch [1614/3000], Loss: 1.0785\n",
      "Epoch [1616/3000], Loss: 1.0771\n",
      "Epoch [1618/3000], Loss: 1.0766\n",
      "Epoch [1620/3000], Loss: 1.0803\n",
      "Epoch [1622/3000], Loss: 1.0802\n",
      "Epoch [1624/3000], Loss: 1.0796\n",
      "Epoch [1626/3000], Loss: 1.0836\n",
      "Epoch [1628/3000], Loss: 1.0787\n",
      "Epoch [1630/3000], Loss: 1.0781\n",
      "Epoch [1632/3000], Loss: 1.0757\n",
      "Epoch [1634/3000], Loss: 1.0777\n",
      "Epoch [1636/3000], Loss: 1.0787\n",
      "Epoch [1638/3000], Loss: 1.0770\n",
      "Epoch [1640/3000], Loss: 1.0809\n",
      "Epoch [1642/3000], Loss: 1.0830\n",
      "Epoch [1644/3000], Loss: 1.0805\n",
      "Epoch [1646/3000], Loss: 1.0818\n",
      "Epoch [1648/3000], Loss: 1.0774\n",
      "Epoch [1650/3000], Loss: 1.0740\n",
      "Epoch [1652/3000], Loss: 1.0776\n",
      "Epoch [1654/3000], Loss: 1.0827\n",
      "Epoch [1656/3000], Loss: 1.0827\n",
      "Epoch [1658/3000], Loss: 1.0772\n",
      "Epoch [1660/3000], Loss: 1.0768\n",
      "Epoch [1662/3000], Loss: 1.0769\n",
      "Epoch [1664/3000], Loss: 1.0805\n",
      "Epoch [1666/3000], Loss: 1.0782\n",
      "Epoch [1668/3000], Loss: 1.0822\n",
      "Epoch [1670/3000], Loss: 1.0839\n",
      "Epoch [1672/3000], Loss: 1.0747\n",
      "Epoch [1674/3000], Loss: 1.0776\n",
      "Epoch [1676/3000], Loss: 1.0779\n",
      "Epoch [1678/3000], Loss: 1.0809\n",
      "Epoch [1680/3000], Loss: 1.0768\n",
      "Epoch [1682/3000], Loss: 1.0761\n",
      "Epoch [1684/3000], Loss: 1.0779\n",
      "Epoch [1686/3000], Loss: 1.0778\n",
      "Epoch [1688/3000], Loss: 1.0791\n",
      "Epoch [1690/3000], Loss: 1.0805\n",
      "Epoch [1692/3000], Loss: 1.0778\n",
      "Epoch [1694/3000], Loss: 1.0826\n",
      "Epoch [1696/3000], Loss: 1.0838\n",
      "Epoch [1698/3000], Loss: 1.0804\n",
      "Epoch [1700/3000], Loss: 1.0789\n",
      "Epoch [1702/3000], Loss: 1.0741\n",
      "Epoch [1704/3000], Loss: 1.0790\n",
      "Epoch [1706/3000], Loss: 1.0779\n",
      "Epoch [1708/3000], Loss: 1.0737\n",
      "Epoch [1710/3000], Loss: 1.0769\n",
      "Epoch [1712/3000], Loss: 1.0707\n",
      "Epoch [1714/3000], Loss: 1.0784\n",
      "Epoch [1716/3000], Loss: 1.0768\n",
      "Epoch [1718/3000], Loss: 1.0764\n",
      "Epoch [1720/3000], Loss: 1.0805\n",
      "Epoch [1722/3000], Loss: 1.0743\n",
      "Epoch [1724/3000], Loss: 1.0782\n",
      "Epoch [1726/3000], Loss: 1.0790\n",
      "Epoch [1728/3000], Loss: 1.0719\n",
      "Epoch [1730/3000], Loss: 1.0726\n",
      "Epoch [1732/3000], Loss: 1.0780\n",
      "Epoch [1734/3000], Loss: 1.0787\n",
      "Epoch [1736/3000], Loss: 1.0820\n",
      "Epoch [1738/3000], Loss: 1.0736\n",
      "Epoch [1740/3000], Loss: 1.0753\n",
      "Epoch [1742/3000], Loss: 1.0803\n",
      "Epoch [1744/3000], Loss: 1.0766\n",
      "Epoch [1746/3000], Loss: 1.0764\n",
      "Epoch [1748/3000], Loss: 1.0718\n",
      "Epoch [1750/3000], Loss: 1.0749\n",
      "Epoch [1752/3000], Loss: 1.0765\n",
      "Epoch [1754/3000], Loss: 1.0812\n",
      "Epoch [1756/3000], Loss: 1.0782\n",
      "Epoch [1758/3000], Loss: 1.0763\n",
      "Epoch [1760/3000], Loss: 1.0770\n",
      "Epoch [1762/3000], Loss: 1.0774\n",
      "Epoch [1764/3000], Loss: 1.0771\n",
      "Epoch [1766/3000], Loss: 1.0749\n",
      "Epoch [1768/3000], Loss: 1.0783\n",
      "Epoch [1770/3000], Loss: 1.0793\n",
      "Epoch [1772/3000], Loss: 1.0845\n",
      "Epoch [1774/3000], Loss: 1.0776\n",
      "Epoch [1776/3000], Loss: 1.0829\n",
      "Epoch [1778/3000], Loss: 1.0778\n",
      "Epoch [1780/3000], Loss: 1.0739\n",
      "Epoch [1782/3000], Loss: 1.0802\n",
      "Epoch [1784/3000], Loss: 1.0778\n",
      "Epoch [1786/3000], Loss: 1.0768\n",
      "Epoch [1788/3000], Loss: 1.0740\n",
      "Epoch [1790/3000], Loss: 1.0781\n",
      "Epoch [1792/3000], Loss: 1.0756\n",
      "Epoch [1794/3000], Loss: 1.0758\n",
      "Epoch [1796/3000], Loss: 1.0800\n",
      "Epoch [1798/3000], Loss: 1.0789\n",
      "Epoch [1800/3000], Loss: 1.0779\n",
      "Epoch [1802/3000], Loss: 1.0778\n",
      "Epoch [1804/3000], Loss: 1.0761\n",
      "Epoch [1806/3000], Loss: 1.0819\n",
      "Epoch [1808/3000], Loss: 1.0760\n",
      "Epoch [1810/3000], Loss: 1.0754\n",
      "Epoch [1812/3000], Loss: 1.0773\n",
      "Epoch [1814/3000], Loss: 1.0822\n",
      "Epoch [1816/3000], Loss: 1.0721\n",
      "Epoch [1818/3000], Loss: 1.0784\n",
      "Epoch [1820/3000], Loss: 1.0779\n",
      "Epoch [1822/3000], Loss: 1.0716\n",
      "Epoch [1824/3000], Loss: 1.0752\n",
      "Epoch [1826/3000], Loss: 1.0774\n",
      "Epoch [1828/3000], Loss: 1.0778\n",
      "Epoch [1830/3000], Loss: 1.0783\n",
      "Epoch [1832/3000], Loss: 1.0772\n",
      "Epoch [1834/3000], Loss: 1.0718\n",
      "Epoch [1836/3000], Loss: 1.0798\n",
      "Epoch [1838/3000], Loss: 1.0743\n",
      "Epoch [1840/3000], Loss: 1.0743\n",
      "Epoch [1842/3000], Loss: 1.0760\n",
      "Epoch [1844/3000], Loss: 1.0802\n",
      "Epoch [1846/3000], Loss: 1.0808\n",
      "Epoch [1848/3000], Loss: 1.0780\n",
      "Epoch [1850/3000], Loss: 1.0737\n",
      "Epoch [1852/3000], Loss: 1.0763\n",
      "Epoch [1854/3000], Loss: 1.0744\n",
      "Epoch [1856/3000], Loss: 1.0772\n",
      "Epoch [1858/3000], Loss: 1.0767\n",
      "Epoch [1860/3000], Loss: 1.0770\n",
      "Epoch [1862/3000], Loss: 1.0738\n",
      "Epoch [1864/3000], Loss: 1.0787\n",
      "Epoch [1866/3000], Loss: 1.0798\n",
      "Epoch [1868/3000], Loss: 1.0778\n",
      "Epoch [1870/3000], Loss: 1.0787\n",
      "Epoch [1872/3000], Loss: 1.0770\n",
      "Epoch [1874/3000], Loss: 1.0804\n",
      "Epoch [1876/3000], Loss: 1.0812\n",
      "Epoch [1878/3000], Loss: 1.0759\n",
      "Epoch [1880/3000], Loss: 1.0774\n",
      "Epoch [1882/3000], Loss: 1.0790\n",
      "Epoch [1884/3000], Loss: 1.0764\n",
      "Epoch [1886/3000], Loss: 1.0749\n",
      "Epoch [1888/3000], Loss: 1.0767\n",
      "Epoch [1890/3000], Loss: 1.0781\n",
      "Epoch [1892/3000], Loss: 1.0822\n",
      "Epoch [1894/3000], Loss: 1.0817\n",
      "Epoch [1896/3000], Loss: 1.0761\n",
      "Epoch [1898/3000], Loss: 1.0786\n",
      "Epoch [1900/3000], Loss: 1.0751\n",
      "Epoch [1902/3000], Loss: 1.0767\n",
      "Epoch [1904/3000], Loss: 1.0799\n",
      "Epoch [1906/3000], Loss: 1.0723\n",
      "Epoch [1908/3000], Loss: 1.0766\n",
      "Epoch [1910/3000], Loss: 1.0780\n",
      "Epoch [1912/3000], Loss: 1.0794\n",
      "Epoch [1914/3000], Loss: 1.0739\n",
      "Epoch [1916/3000], Loss: 1.0789\n",
      "Epoch [1918/3000], Loss: 1.0736\n",
      "Epoch [1920/3000], Loss: 1.0767\n",
      "Epoch [1922/3000], Loss: 1.0782\n",
      "Epoch [1924/3000], Loss: 1.0778\n",
      "Epoch [1926/3000], Loss: 1.0735\n",
      "Epoch [1928/3000], Loss: 1.0770\n",
      "Epoch [1930/3000], Loss: 1.0712\n",
      "Epoch [1932/3000], Loss: 1.0808\n",
      "Epoch [1934/3000], Loss: 1.0758\n",
      "Epoch [1936/3000], Loss: 1.0846\n",
      "Epoch [1938/3000], Loss: 1.0780\n",
      "Epoch [1940/3000], Loss: 1.0770\n",
      "Epoch [1942/3000], Loss: 1.0778\n",
      "Epoch [1944/3000], Loss: 1.0750\n",
      "Epoch [1946/3000], Loss: 1.0778\n",
      "Epoch [1948/3000], Loss: 1.0728\n",
      "Epoch [1950/3000], Loss: 1.0725\n",
      "Epoch [1952/3000], Loss: 1.0739\n",
      "Epoch [1954/3000], Loss: 1.0765\n",
      "Epoch [1956/3000], Loss: 1.0783\n",
      "Epoch [1958/3000], Loss: 1.0797\n",
      "Epoch [1960/3000], Loss: 1.0840\n",
      "Epoch [1962/3000], Loss: 1.0727\n",
      "Epoch [1964/3000], Loss: 1.0760\n",
      "Epoch [1966/3000], Loss: 1.0770\n",
      "Epoch [1968/3000], Loss: 1.0808\n",
      "Epoch [1970/3000], Loss: 1.0785\n",
      "Epoch [1972/3000], Loss: 1.0777\n",
      "Epoch [1974/3000], Loss: 1.0754\n",
      "Epoch [1976/3000], Loss: 1.0791\n",
      "Epoch [1978/3000], Loss: 1.0777\n",
      "Epoch [1980/3000], Loss: 1.0747\n",
      "Epoch [1982/3000], Loss: 1.0752\n",
      "Epoch [1984/3000], Loss: 1.0764\n",
      "Epoch [1986/3000], Loss: 1.0768\n",
      "Epoch [1988/3000], Loss: 1.0825\n",
      "Epoch [1990/3000], Loss: 1.0773\n",
      "Epoch [1992/3000], Loss: 1.0779\n",
      "Epoch [1994/3000], Loss: 1.0782\n",
      "Epoch [1996/3000], Loss: 1.0759\n",
      "Epoch [1998/3000], Loss: 1.0688\n",
      "Epoch [2000/3000], Loss: 1.0808\n",
      "Epoch [2002/3000], Loss: 1.0773\n",
      "Epoch [2004/3000], Loss: 1.0726\n",
      "Epoch [2006/3000], Loss: 1.0733\n",
      "Epoch [2008/3000], Loss: 1.0783\n",
      "Epoch [2010/3000], Loss: 1.0772\n",
      "Epoch [2012/3000], Loss: 1.0783\n",
      "Epoch [2014/3000], Loss: 1.0795\n",
      "Epoch [2016/3000], Loss: 1.0782\n",
      "Epoch [2018/3000], Loss: 1.0761\n",
      "Epoch [2020/3000], Loss: 1.0755\n",
      "Epoch [2022/3000], Loss: 1.0768\n",
      "Epoch [2024/3000], Loss: 1.0767\n",
      "Epoch [2026/3000], Loss: 1.0734\n",
      "Epoch [2028/3000], Loss: 1.0777\n",
      "Epoch [2030/3000], Loss: 1.0788\n",
      "Epoch [2032/3000], Loss: 1.0774\n",
      "Epoch [2034/3000], Loss: 1.0740\n",
      "Epoch [2036/3000], Loss: 1.0727\n",
      "Epoch [2038/3000], Loss: 1.0817\n",
      "Epoch [2040/3000], Loss: 1.0749\n",
      "Epoch [2042/3000], Loss: 1.0756\n",
      "Epoch [2044/3000], Loss: 1.0779\n",
      "Epoch [2046/3000], Loss: 1.0792\n",
      "Epoch [2048/3000], Loss: 1.0763\n",
      "Epoch [2050/3000], Loss: 1.0771\n",
      "Epoch [2052/3000], Loss: 1.0749\n",
      "Epoch [2054/3000], Loss: 1.0749\n",
      "Epoch [2056/3000], Loss: 1.0742\n",
      "Epoch [2058/3000], Loss: 1.0772\n",
      "Epoch [2060/3000], Loss: 1.0807\n",
      "Epoch [2062/3000], Loss: 1.0784\n",
      "Epoch [2064/3000], Loss: 1.0751\n",
      "Epoch [2066/3000], Loss: 1.0744\n",
      "Epoch [2068/3000], Loss: 1.0772\n",
      "Epoch [2070/3000], Loss: 1.0797\n",
      "Epoch [2072/3000], Loss: 1.0759\n",
      "Epoch [2074/3000], Loss: 1.0787\n",
      "Epoch [2076/3000], Loss: 1.0703\n",
      "Epoch [2078/3000], Loss: 1.0757\n",
      "Epoch [2080/3000], Loss: 1.0776\n",
      "Epoch [2082/3000], Loss: 1.0740\n",
      "Epoch [2084/3000], Loss: 1.0751\n",
      "Epoch [2086/3000], Loss: 1.0732\n",
      "Epoch [2088/3000], Loss: 1.0739\n",
      "Epoch [2090/3000], Loss: 1.0788\n",
      "Epoch [2092/3000], Loss: 1.0787\n",
      "Epoch [2094/3000], Loss: 1.0775\n",
      "Epoch [2096/3000], Loss: 1.0758\n",
      "Epoch [2098/3000], Loss: 1.0801\n",
      "Epoch [2100/3000], Loss: 1.0740\n",
      "Epoch [2102/3000], Loss: 1.0747\n",
      "Epoch [2104/3000], Loss: 1.0765\n",
      "Epoch [2106/3000], Loss: 1.0758\n",
      "Epoch [2108/3000], Loss: 1.0744\n",
      "Epoch [2110/3000], Loss: 1.0727\n",
      "Epoch [2112/3000], Loss: 1.0757\n",
      "Epoch [2114/3000], Loss: 1.0785\n",
      "Epoch [2116/3000], Loss: 1.0826\n",
      "Epoch [2118/3000], Loss: 1.0768\n",
      "Epoch [2120/3000], Loss: 1.0805\n",
      "Epoch [2122/3000], Loss: 1.0753\n",
      "Epoch [2124/3000], Loss: 1.0821\n",
      "Epoch [2126/3000], Loss: 1.0741\n",
      "Epoch [2128/3000], Loss: 1.0791\n",
      "Epoch [2130/3000], Loss: 1.0759\n",
      "Epoch [2132/3000], Loss: 1.0768\n",
      "Epoch [2134/3000], Loss: 1.0759\n",
      "Epoch [2136/3000], Loss: 1.0744\n",
      "Epoch [2138/3000], Loss: 1.0765\n",
      "Epoch [2140/3000], Loss: 1.0726\n",
      "Epoch [2142/3000], Loss: 1.0749\n",
      "Epoch [2144/3000], Loss: 1.0801\n",
      "Epoch [2146/3000], Loss: 1.0781\n",
      "Epoch [2148/3000], Loss: 1.0763\n",
      "Epoch [2150/3000], Loss: 1.0792\n",
      "Epoch [2152/3000], Loss: 1.0786\n",
      "Epoch [2154/3000], Loss: 1.0724\n",
      "Epoch [2156/3000], Loss: 1.0800\n",
      "Epoch [2158/3000], Loss: 1.0777\n",
      "Epoch [2160/3000], Loss: 1.0748\n",
      "Epoch [2162/3000], Loss: 1.0778\n",
      "Epoch [2164/3000], Loss: 1.0773\n",
      "Epoch [2166/3000], Loss: 1.0822\n",
      "Epoch [2168/3000], Loss: 1.0756\n",
      "Epoch [2170/3000], Loss: 1.0739\n",
      "Epoch [2172/3000], Loss: 1.0792\n",
      "Epoch [2174/3000], Loss: 1.0763\n",
      "Epoch [2176/3000], Loss: 1.0760\n",
      "Epoch [2178/3000], Loss: 1.0790\n",
      "Epoch [2180/3000], Loss: 1.0792\n",
      "Epoch [2182/3000], Loss: 1.0762\n",
      "Epoch [2184/3000], Loss: 1.0785\n",
      "Epoch [2186/3000], Loss: 1.0728\n",
      "Epoch [2188/3000], Loss: 1.0778\n",
      "Epoch [2190/3000], Loss: 1.0790\n",
      "Epoch [2192/3000], Loss: 1.0766\n",
      "Epoch [2194/3000], Loss: 1.0747\n",
      "Epoch [2196/3000], Loss: 1.0773\n",
      "Epoch [2198/3000], Loss: 1.0701\n",
      "Epoch [2200/3000], Loss: 1.0745\n",
      "Epoch [2202/3000], Loss: 1.0766\n",
      "Epoch [2204/3000], Loss: 1.0718\n",
      "Epoch [2206/3000], Loss: 1.0755\n",
      "Epoch [2208/3000], Loss: 1.0765\n",
      "Epoch [2210/3000], Loss: 1.0751\n",
      "Epoch [2212/3000], Loss: 1.0729\n",
      "Epoch [2214/3000], Loss: 1.0795\n",
      "Epoch [2216/3000], Loss: 1.0776\n",
      "Epoch [2218/3000], Loss: 1.0758\n",
      "Epoch [2220/3000], Loss: 1.0817\n",
      "Epoch [2222/3000], Loss: 1.0726\n",
      "Epoch [2224/3000], Loss: 1.0768\n",
      "Epoch [2226/3000], Loss: 1.0776\n",
      "Epoch [2228/3000], Loss: 1.0756\n",
      "Epoch [2230/3000], Loss: 1.0741\n",
      "Epoch [2232/3000], Loss: 1.0725\n",
      "Epoch [2234/3000], Loss: 1.0767\n",
      "Epoch [2236/3000], Loss: 1.0811\n",
      "Epoch [2238/3000], Loss: 1.0743\n",
      "Epoch [2240/3000], Loss: 1.0746\n",
      "Epoch [2242/3000], Loss: 1.0731\n",
      "Epoch [2244/3000], Loss: 1.0767\n",
      "Epoch [2246/3000], Loss: 1.0777\n",
      "Epoch [2248/3000], Loss: 1.0752\n",
      "Epoch [2250/3000], Loss: 1.0681\n",
      "Epoch [2252/3000], Loss: 1.0746\n",
      "Epoch [2254/3000], Loss: 1.0738\n",
      "Epoch [2256/3000], Loss: 1.0711\n",
      "Epoch [2258/3000], Loss: 1.0788\n",
      "Epoch [2260/3000], Loss: 1.0761\n",
      "Epoch [2262/3000], Loss: 1.0760\n",
      "Epoch [2264/3000], Loss: 1.0709\n",
      "Epoch [2266/3000], Loss: 1.0749\n",
      "Epoch [2268/3000], Loss: 1.0765\n",
      "Epoch [2270/3000], Loss: 1.0729\n",
      "Epoch [2272/3000], Loss: 1.0727\n",
      "Epoch [2274/3000], Loss: 1.0778\n",
      "Epoch [2276/3000], Loss: 1.0762\n",
      "Epoch [2278/3000], Loss: 1.0794\n",
      "Epoch [2280/3000], Loss: 1.0757\n",
      "Epoch [2282/3000], Loss: 1.0751\n",
      "Epoch [2284/3000], Loss: 1.0768\n",
      "Epoch [2286/3000], Loss: 1.0769\n",
      "Epoch [2288/3000], Loss: 1.0721\n",
      "Epoch [2290/3000], Loss: 1.0772\n",
      "Epoch [2292/3000], Loss: 1.0711\n",
      "Epoch [2294/3000], Loss: 1.0775\n",
      "Epoch [2296/3000], Loss: 1.0756\n",
      "Epoch [2298/3000], Loss: 1.0764\n",
      "Epoch [2300/3000], Loss: 1.0841\n",
      "Epoch [2302/3000], Loss: 1.0710\n",
      "Epoch [2304/3000], Loss: 1.0744\n",
      "Epoch [2306/3000], Loss: 1.0794\n",
      "Epoch [2308/3000], Loss: 1.0756\n",
      "Epoch [2310/3000], Loss: 1.0828\n",
      "Epoch [2312/3000], Loss: 1.0783\n",
      "Epoch [2314/3000], Loss: 1.0740\n",
      "Epoch [2316/3000], Loss: 1.0742\n",
      "Epoch [2318/3000], Loss: 1.0781\n",
      "Epoch [2320/3000], Loss: 1.0736\n",
      "Epoch [2322/3000], Loss: 1.0777\n",
      "Epoch [2324/3000], Loss: 1.0736\n",
      "Epoch [2326/3000], Loss: 1.0764\n",
      "Epoch [2328/3000], Loss: 1.0778\n",
      "Epoch [2330/3000], Loss: 1.0715\n",
      "Epoch [2332/3000], Loss: 1.0781\n",
      "Epoch [2334/3000], Loss: 1.0754\n",
      "Epoch [2336/3000], Loss: 1.0819\n",
      "Epoch [2338/3000], Loss: 1.0738\n",
      "Epoch [2340/3000], Loss: 1.0829\n",
      "Epoch [2342/3000], Loss: 1.0745\n",
      "Epoch [2344/3000], Loss: 1.0775\n",
      "Epoch [2346/3000], Loss: 1.0695\n",
      "Epoch [2348/3000], Loss: 1.0766\n",
      "Epoch [2350/3000], Loss: 1.0756\n",
      "Epoch [2352/3000], Loss: 1.0752\n",
      "Epoch [2354/3000], Loss: 1.0752\n",
      "Epoch [2356/3000], Loss: 1.0731\n",
      "Epoch [2358/3000], Loss: 1.0697\n",
      "Epoch [2360/3000], Loss: 1.0724\n",
      "Epoch [2362/3000], Loss: 1.0781\n",
      "Epoch [2364/3000], Loss: 1.0714\n",
      "Epoch [2366/3000], Loss: 1.0756\n",
      "Epoch [2368/3000], Loss: 1.0783\n",
      "Epoch [2370/3000], Loss: 1.0707\n",
      "Epoch [2372/3000], Loss: 1.0812\n",
      "Epoch [2374/3000], Loss: 1.0725\n",
      "Epoch [2376/3000], Loss: 1.0766\n",
      "Epoch [2378/3000], Loss: 1.0744\n",
      "Epoch [2380/3000], Loss: 1.0793\n",
      "Epoch [2382/3000], Loss: 1.0797\n",
      "Epoch [2384/3000], Loss: 1.0723\n",
      "Epoch [2386/3000], Loss: 1.0757\n",
      "Epoch [2388/3000], Loss: 1.0755\n",
      "Epoch [2390/3000], Loss: 1.0748\n",
      "Epoch [2392/3000], Loss: 1.0764\n",
      "Epoch [2394/3000], Loss: 1.0772\n",
      "Epoch [2396/3000], Loss: 1.0792\n",
      "Epoch [2398/3000], Loss: 1.0754\n",
      "Epoch [2400/3000], Loss: 1.0735\n",
      "Epoch [2402/3000], Loss: 1.0707\n",
      "Epoch [2404/3000], Loss: 1.0756\n",
      "Epoch [2406/3000], Loss: 1.0763\n",
      "Epoch [2408/3000], Loss: 1.0777\n",
      "Epoch [2410/3000], Loss: 1.0721\n",
      "Epoch [2412/3000], Loss: 1.0742\n",
      "Epoch [2414/3000], Loss: 1.0753\n",
      "Epoch [2416/3000], Loss: 1.0829\n",
      "Epoch [2418/3000], Loss: 1.0780\n",
      "Epoch [2420/3000], Loss: 1.0789\n",
      "Epoch [2422/3000], Loss: 1.0756\n",
      "Epoch [2424/3000], Loss: 1.0807\n",
      "Epoch [2426/3000], Loss: 1.0730\n",
      "Epoch [2428/3000], Loss: 1.0722\n",
      "Epoch [2430/3000], Loss: 1.0741\n",
      "Epoch [2432/3000], Loss: 1.0766\n",
      "Epoch [2434/3000], Loss: 1.0747\n",
      "Epoch [2436/3000], Loss: 1.0801\n",
      "Epoch [2438/3000], Loss: 1.0721\n",
      "Epoch [2440/3000], Loss: 1.0725\n",
      "Epoch [2442/3000], Loss: 1.0725\n",
      "Epoch [2444/3000], Loss: 1.0666\n",
      "Epoch [2446/3000], Loss: 1.0769\n",
      "Epoch [2448/3000], Loss: 1.0764\n",
      "Epoch [2450/3000], Loss: 1.0716\n",
      "Epoch [2452/3000], Loss: 1.0734\n",
      "Epoch [2454/3000], Loss: 1.0758\n",
      "Epoch [2456/3000], Loss: 1.0717\n",
      "Epoch [2458/3000], Loss: 1.0698\n",
      "Epoch [2460/3000], Loss: 1.0737\n",
      "Epoch [2462/3000], Loss: 1.0769\n",
      "Epoch [2464/3000], Loss: 1.0736\n",
      "Epoch [2466/3000], Loss: 1.0741\n",
      "Epoch [2468/3000], Loss: 1.0792\n",
      "Epoch [2470/3000], Loss: 1.0761\n",
      "Epoch [2472/3000], Loss: 1.0746\n",
      "Epoch [2474/3000], Loss: 1.0733\n",
      "Epoch [2476/3000], Loss: 1.0735\n",
      "Epoch [2478/3000], Loss: 1.0748\n",
      "Epoch [2480/3000], Loss: 1.0720\n",
      "Epoch [2482/3000], Loss: 1.0784\n",
      "Epoch [2484/3000], Loss: 1.0701\n",
      "Epoch [2486/3000], Loss: 1.0707\n",
      "Epoch [2488/3000], Loss: 1.0774\n",
      "Epoch [2490/3000], Loss: 1.0706\n",
      "Epoch [2492/3000], Loss: 1.0753\n",
      "Epoch [2494/3000], Loss: 1.0731\n",
      "Epoch [2496/3000], Loss: 1.0768\n",
      "Epoch [2498/3000], Loss: 1.0724\n",
      "Epoch [2500/3000], Loss: 1.0730\n",
      "Epoch [2502/3000], Loss: 1.0757\n",
      "Epoch [2504/3000], Loss: 1.0769\n",
      "Epoch [2506/3000], Loss: 1.0741\n",
      "Epoch [2508/3000], Loss: 1.0754\n",
      "Epoch [2510/3000], Loss: 1.0728\n",
      "Epoch [2512/3000], Loss: 1.0734\n",
      "Epoch [2514/3000], Loss: 1.0702\n",
      "Epoch [2516/3000], Loss: 1.0691\n",
      "Epoch [2518/3000], Loss: 1.0773\n",
      "Epoch [2520/3000], Loss: 1.0747\n",
      "Epoch [2522/3000], Loss: 1.0759\n",
      "Epoch [2524/3000], Loss: 1.0760\n",
      "Epoch [2526/3000], Loss: 1.0791\n",
      "Epoch [2528/3000], Loss: 1.0791\n",
      "Epoch [2530/3000], Loss: 1.0731\n",
      "Epoch [2532/3000], Loss: 1.0773\n",
      "Epoch [2534/3000], Loss: 1.0774\n",
      "Epoch [2536/3000], Loss: 1.0758\n",
      "Epoch [2538/3000], Loss: 1.0724\n",
      "Epoch [2540/3000], Loss: 1.0753\n",
      "Epoch [2542/3000], Loss: 1.0728\n",
      "Epoch [2544/3000], Loss: 1.0731\n",
      "Epoch [2546/3000], Loss: 1.0780\n",
      "Epoch [2548/3000], Loss: 1.0726\n",
      "Epoch [2550/3000], Loss: 1.0761\n",
      "Epoch [2552/3000], Loss: 1.0757\n",
      "Epoch [2554/3000], Loss: 1.0743\n",
      "Epoch [2556/3000], Loss: 1.0804\n",
      "Epoch [2558/3000], Loss: 1.0779\n",
      "Epoch [2560/3000], Loss: 1.0775\n",
      "Epoch [2562/3000], Loss: 1.0775\n",
      "Epoch [2564/3000], Loss: 1.0754\n",
      "Epoch [2566/3000], Loss: 1.0765\n",
      "Epoch [2568/3000], Loss: 1.0732\n",
      "Epoch [2570/3000], Loss: 1.0758\n",
      "Epoch [2572/3000], Loss: 1.0718\n",
      "Epoch [2574/3000], Loss: 1.0749\n",
      "Epoch [2576/3000], Loss: 1.0752\n",
      "Epoch [2578/3000], Loss: 1.0776\n",
      "Epoch [2580/3000], Loss: 1.0715\n",
      "Epoch [2582/3000], Loss: 1.0728\n",
      "Epoch [2584/3000], Loss: 1.0829\n",
      "Epoch [2586/3000], Loss: 1.0757\n",
      "Epoch [2588/3000], Loss: 1.0746\n",
      "Epoch [2590/3000], Loss: 1.0738\n",
      "Epoch [2592/3000], Loss: 1.0781\n",
      "Epoch [2594/3000], Loss: 1.0717\n",
      "Epoch [2596/3000], Loss: 1.0767\n",
      "Epoch [2598/3000], Loss: 1.0767\n",
      "Epoch [2600/3000], Loss: 1.0791\n",
      "Epoch [2602/3000], Loss: 1.0731\n",
      "Epoch [2604/3000], Loss: 1.0771\n",
      "Epoch [2606/3000], Loss: 1.0742\n",
      "Epoch [2608/3000], Loss: 1.0719\n",
      "Epoch [2610/3000], Loss: 1.0756\n",
      "Epoch [2612/3000], Loss: 1.0719\n",
      "Epoch [2614/3000], Loss: 1.0746\n",
      "Epoch [2616/3000], Loss: 1.0786\n",
      "Epoch [2618/3000], Loss: 1.0742\n",
      "Epoch [2620/3000], Loss: 1.0794\n",
      "Epoch [2622/3000], Loss: 1.0721\n",
      "Epoch [2624/3000], Loss: 1.0749\n",
      "Epoch [2626/3000], Loss: 1.0792\n",
      "Epoch [2628/3000], Loss: 1.0688\n",
      "Epoch [2630/3000], Loss: 1.0759\n",
      "Epoch [2632/3000], Loss: 1.0755\n",
      "Epoch [2634/3000], Loss: 1.0739\n",
      "Epoch [2636/3000], Loss: 1.0775\n",
      "Epoch [2638/3000], Loss: 1.0734\n",
      "Epoch [2640/3000], Loss: 1.0714\n",
      "Epoch [2642/3000], Loss: 1.0736\n",
      "Epoch [2644/3000], Loss: 1.0720\n",
      "Epoch [2646/3000], Loss: 1.0768\n",
      "Epoch [2648/3000], Loss: 1.0716\n",
      "Epoch [2650/3000], Loss: 1.0758\n",
      "Epoch [2652/3000], Loss: 1.0755\n",
      "Epoch [2654/3000], Loss: 1.0720\n",
      "Epoch [2656/3000], Loss: 1.0726\n",
      "Epoch [2658/3000], Loss: 1.0756\n",
      "Epoch [2660/3000], Loss: 1.0726\n",
      "Epoch [2662/3000], Loss: 1.0760\n",
      "Epoch [2664/3000], Loss: 1.0729\n",
      "Epoch [2666/3000], Loss: 1.0791\n",
      "Epoch [2668/3000], Loss: 1.0756\n",
      "Epoch [2670/3000], Loss: 1.0691\n",
      "Epoch [2672/3000], Loss: 1.0786\n",
      "Epoch [2674/3000], Loss: 1.0767\n",
      "Epoch [2676/3000], Loss: 1.0755\n",
      "Epoch [2678/3000], Loss: 1.0774\n",
      "Epoch [2680/3000], Loss: 1.0728\n",
      "Epoch [2682/3000], Loss: 1.0714\n",
      "Epoch [2684/3000], Loss: 1.0683\n",
      "Epoch [2686/3000], Loss: 1.0726\n",
      "Epoch [2688/3000], Loss: 1.0728\n",
      "Epoch [2690/3000], Loss: 1.0694\n",
      "Epoch [2692/3000], Loss: 1.0699\n",
      "Epoch [2694/3000], Loss: 1.0692\n",
      "Epoch [2696/3000], Loss: 1.0721\n",
      "Epoch [2698/3000], Loss: 1.0738\n",
      "Epoch [2700/3000], Loss: 1.0735\n",
      "Epoch [2702/3000], Loss: 1.0734\n",
      "Epoch [2704/3000], Loss: 1.0715\n",
      "Epoch [2706/3000], Loss: 1.0745\n",
      "Epoch [2708/3000], Loss: 1.0732\n",
      "Epoch [2710/3000], Loss: 1.0700\n",
      "Epoch [2712/3000], Loss: 1.0731\n",
      "Epoch [2714/3000], Loss: 1.0790\n",
      "Epoch [2716/3000], Loss: 1.0724\n",
      "Epoch [2718/3000], Loss: 1.0749\n",
      "Epoch [2720/3000], Loss: 1.0734\n",
      "Epoch [2722/3000], Loss: 1.0725\n",
      "Epoch [2724/3000], Loss: 1.0753\n",
      "Epoch [2726/3000], Loss: 1.0770\n",
      "Epoch [2728/3000], Loss: 1.0763\n",
      "Epoch [2730/3000], Loss: 1.0716\n",
      "Epoch [2732/3000], Loss: 1.0753\n",
      "Epoch [2734/3000], Loss: 1.0707\n",
      "Epoch [2736/3000], Loss: 1.0778\n",
      "Epoch [2738/3000], Loss: 1.0730\n",
      "Epoch [2740/3000], Loss: 1.0725\n",
      "Epoch [2742/3000], Loss: 1.0767\n",
      "Epoch [2744/3000], Loss: 1.0724\n",
      "Epoch [2746/3000], Loss: 1.0740\n",
      "Epoch [2748/3000], Loss: 1.0708\n",
      "Epoch [2750/3000], Loss: 1.0794\n",
      "Epoch [2752/3000], Loss: 1.0707\n",
      "Epoch [2754/3000], Loss: 1.0749\n",
      "Epoch [2756/3000], Loss: 1.0718\n",
      "Epoch [2758/3000], Loss: 1.0774\n",
      "Epoch [2760/3000], Loss: 1.0798\n",
      "Epoch [2762/3000], Loss: 1.0752\n",
      "Epoch [2764/3000], Loss: 1.0734\n",
      "Epoch [2766/3000], Loss: 1.0737\n",
      "Epoch [2768/3000], Loss: 1.0753\n",
      "Epoch [2770/3000], Loss: 1.0740\n",
      "Epoch [2772/3000], Loss: 1.0786\n",
      "Epoch [2774/3000], Loss: 1.0779\n",
      "Epoch [2776/3000], Loss: 1.0797\n",
      "Epoch [2778/3000], Loss: 1.0723\n",
      "Epoch [2780/3000], Loss: 1.0680\n",
      "Epoch [2782/3000], Loss: 1.0737\n",
      "Epoch [2784/3000], Loss: 1.0749\n",
      "Epoch [2786/3000], Loss: 1.0739\n",
      "Epoch [2788/3000], Loss: 1.0719\n",
      "Epoch [2790/3000], Loss: 1.0753\n",
      "Epoch [2792/3000], Loss: 1.0751\n",
      "Epoch [2794/3000], Loss: 1.0711\n",
      "Epoch [2796/3000], Loss: 1.0728\n",
      "Epoch [2798/3000], Loss: 1.0729\n",
      "Epoch [2800/3000], Loss: 1.0782\n",
      "Epoch [2802/3000], Loss: 1.0706\n",
      "Epoch [2804/3000], Loss: 1.0682\n",
      "Epoch [2806/3000], Loss: 1.0725\n",
      "Epoch [2808/3000], Loss: 1.0757\n",
      "Epoch [2810/3000], Loss: 1.0750\n",
      "Epoch [2812/3000], Loss: 1.0719\n",
      "Epoch [2814/3000], Loss: 1.0777\n",
      "Epoch [2816/3000], Loss: 1.0741\n",
      "Epoch [2818/3000], Loss: 1.0794\n",
      "Epoch [2820/3000], Loss: 1.0709\n",
      "Epoch [2822/3000], Loss: 1.0783\n",
      "Epoch [2824/3000], Loss: 1.0772\n",
      "Epoch [2826/3000], Loss: 1.0759\n",
      "Epoch [2828/3000], Loss: 1.0722\n",
      "Epoch [2830/3000], Loss: 1.0740\n",
      "Epoch [2832/3000], Loss: 1.0724\n",
      "Epoch [2834/3000], Loss: 1.0717\n",
      "Epoch [2836/3000], Loss: 1.0709\n",
      "Epoch [2838/3000], Loss: 1.0752\n",
      "Epoch [2840/3000], Loss: 1.0689\n",
      "Epoch [2842/3000], Loss: 1.0698\n",
      "Epoch [2844/3000], Loss: 1.0725\n",
      "Epoch [2846/3000], Loss: 1.0785\n",
      "Epoch [2848/3000], Loss: 1.0744\n",
      "Epoch [2850/3000], Loss: 1.0722\n",
      "Epoch [2852/3000], Loss: 1.0716\n",
      "Epoch [2854/3000], Loss: 1.0716\n",
      "Epoch [2856/3000], Loss: 1.0722\n",
      "Epoch [2858/3000], Loss: 1.0731\n",
      "Epoch [2860/3000], Loss: 1.0746\n",
      "Epoch [2862/3000], Loss: 1.0740\n",
      "Epoch [2864/3000], Loss: 1.0733\n",
      "Epoch [2866/3000], Loss: 1.0749\n",
      "Epoch [2868/3000], Loss: 1.0729\n",
      "Epoch [2870/3000], Loss: 1.0729\n",
      "Epoch [2872/3000], Loss: 1.0754\n",
      "Epoch [2874/3000], Loss: 1.0756\n",
      "Epoch [2876/3000], Loss: 1.0749\n",
      "Epoch [2878/3000], Loss: 1.0662\n",
      "Epoch [2880/3000], Loss: 1.0761\n",
      "Epoch [2882/3000], Loss: 1.0748\n",
      "Epoch [2884/3000], Loss: 1.0732\n",
      "Epoch [2886/3000], Loss: 1.0732\n",
      "Epoch [2888/3000], Loss: 1.0716\n",
      "Epoch [2890/3000], Loss: 1.0727\n",
      "Epoch [2892/3000], Loss: 1.0734\n",
      "Epoch [2894/3000], Loss: 1.0734\n",
      "Epoch [2896/3000], Loss: 1.0698\n",
      "Epoch [2898/3000], Loss: 1.0730\n",
      "Epoch [2900/3000], Loss: 1.0705\n",
      "Epoch [2902/3000], Loss: 1.0766\n",
      "Epoch [2904/3000], Loss: 1.0718\n",
      "Epoch [2906/3000], Loss: 1.0736\n",
      "Epoch [2908/3000], Loss: 1.0739\n",
      "Epoch [2910/3000], Loss: 1.0676\n",
      "Epoch [2912/3000], Loss: 1.0729\n",
      "Epoch [2914/3000], Loss: 1.0709\n",
      "Epoch [2916/3000], Loss: 1.0707\n",
      "Epoch [2918/3000], Loss: 1.0733\n",
      "Epoch [2920/3000], Loss: 1.0731\n",
      "Epoch [2922/3000], Loss: 1.0754\n",
      "Epoch [2924/3000], Loss: 1.0724\n",
      "Epoch [2926/3000], Loss: 1.0736\n",
      "Epoch [2928/3000], Loss: 1.0700\n",
      "Epoch [2930/3000], Loss: 1.0735\n",
      "Epoch [2932/3000], Loss: 1.0730\n",
      "Epoch [2934/3000], Loss: 1.0746\n",
      "Epoch [2936/3000], Loss: 1.0695\n",
      "Epoch [2938/3000], Loss: 1.0740\n",
      "Epoch [2940/3000], Loss: 1.0650\n",
      "Epoch [2942/3000], Loss: 1.0765\n",
      "Epoch [2944/3000], Loss: 1.0724\n",
      "Epoch [2946/3000], Loss: 1.0718\n",
      "Epoch [2948/3000], Loss: 1.0736\n",
      "Epoch [2950/3000], Loss: 1.0725\n",
      "Epoch [2952/3000], Loss: 1.0649\n",
      "Epoch [2954/3000], Loss: 1.0730\n",
      "Epoch [2956/3000], Loss: 1.0748\n",
      "Epoch [2958/3000], Loss: 1.0763\n",
      "Epoch [2960/3000], Loss: 1.0784\n",
      "Epoch [2962/3000], Loss: 1.0779\n",
      "Epoch [2964/3000], Loss: 1.0739\n",
      "Epoch [2966/3000], Loss: 1.0727\n",
      "Epoch [2968/3000], Loss: 1.0776\n",
      "Epoch [2970/3000], Loss: 1.0762\n",
      "Epoch [2972/3000], Loss: 1.0742\n",
      "Epoch [2974/3000], Loss: 1.0730\n",
      "Epoch [2976/3000], Loss: 1.0753\n",
      "Epoch [2978/3000], Loss: 1.0776\n",
      "Epoch [2980/3000], Loss: 1.0755\n",
      "Epoch [2982/3000], Loss: 1.0718\n",
      "Epoch [2984/3000], Loss: 1.0715\n",
      "Epoch [2986/3000], Loss: 1.0741\n",
      "Epoch [2988/3000], Loss: 1.0780\n",
      "Epoch [2990/3000], Loss: 1.0755\n",
      "Epoch [2992/3000], Loss: 1.0715\n",
      "Epoch [2994/3000], Loss: 1.0754\n",
      "Epoch [2996/3000], Loss: 1.0760\n",
      "Epoch [2998/3000], Loss: 1.0737\n",
      "Epoch [3000/3000], Loss: 1.0772\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"meta_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save jit model\n",
    "model = model.to(\"cpu\")\n",
    "torch.jit.save(torch.jit.script(model), \"meta_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.8402\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_test.cpu(), predicted.cpu())\n",
    "    print(f\"Ensemble Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4179e-09, 1.5859e-08, 4.9914e-09, 1.0417e-08, 5.3423e-08, 2.8951e-09,\n",
       "        3.8859e-09, 1.0728e-08, 1.2620e-10, 8.3270e-06, 5.5221e-05, 1.9113e-11,\n",
       "        1.5181e-08, 2.2436e-06, 9.9454e-01, 8.7898e-07, 2.4719e-09, 4.9289e-06,\n",
       "        2.5505e-09, 2.0129e-08, 1.2814e-06, 4.1809e-09, 1.5721e-07, 1.9579e-09,\n",
       "        1.3403e-08, 1.0716e-08, 8.5534e-09, 3.9499e-07, 5.9987e-08, 1.9226e-08,\n",
       "        8.0016e-09, 1.3039e-07, 1.9754e-10, 6.1245e-04, 3.9653e-09, 2.1754e-08,\n",
       "        3.6285e-09, 5.0791e-05, 3.3474e-07, 1.9684e-06, 2.1195e-04, 2.7917e-06,\n",
       "        1.9002e-07, 3.9435e-06, 4.4361e-06, 1.5169e-08, 8.9984e-08, 1.0855e-10,\n",
       "        3.2561e-07, 9.3178e-09, 1.2788e-07, 4.4158e-09, 9.3306e-09, 6.3601e-05,\n",
       "        3.9292e-11, 1.3248e-06, 6.6989e-11, 6.2909e-06, 3.5056e-09, 1.8391e-04,\n",
       "        1.1768e-05, 4.3406e-11, 5.0470e-05, 3.7790e-07, 3.0548e-08, 2.8209e-04,\n",
       "        2.1308e-10, 1.4738e-08, 1.8641e-09, 1.0149e-10, 2.7075e-07, 7.4253e-05,\n",
       "        9.5298e-09, 2.1444e-07, 1.7783e-07, 4.8169e-10, 3.4341e-06, 3.6768e-10,\n",
       "        1.0371e-08, 1.9410e-09, 4.7322e-06, 8.8427e-08, 1.0217e-05, 3.9075e-11,\n",
       "        4.9709e-08, 1.3079e-08, 2.7114e-07, 1.1794e-09, 3.4679e-09, 1.2645e-08,\n",
       "        1.3004e-08, 1.0836e-09, 2.1114e-08, 1.4365e-09, 4.5058e-09, 3.7455e-09,\n",
       "        8.8233e-08, 1.1757e-09, 5.9331e-05, 6.5649e-10, 4.8168e-06, 5.5937e-08,\n",
       "        9.2230e-09, 2.1565e-09, 4.9933e-07, 3.8726e-06, 2.3317e-03, 1.3434e-04,\n",
       "        1.7786e-10, 2.8627e-10, 3.3691e-10, 3.2296e-08, 1.3027e-09, 6.2623e-10,\n",
       "        3.8836e-11, 2.5296e-06, 7.4007e-09, 4.1677e-10, 8.8257e-11, 8.1212e-11,\n",
       "        3.6992e-07, 1.4143e-08, 4.2303e-09, 1.9461e-09, 2.6413e-08, 1.3881e-07,\n",
       "        1.7159e-06, 2.6455e-09, 2.4415e-08, 9.3881e-08, 8.2826e-09, 3.9538e-10,\n",
       "        1.1891e-05, 1.7171e-10, 8.5311e-08, 1.7688e-06, 1.9155e-07, 7.1189e-10,\n",
       "        2.1609e-06, 3.5558e-09, 1.4964e-07, 5.4747e-04, 3.9119e-06, 2.8207e-06,\n",
       "        1.7281e-08, 1.6941e-09, 8.9397e-08, 2.4675e-09, 3.9699e-11, 6.1236e-09,\n",
       "        4.9702e-10, 2.6094e-07, 3.1475e-09, 2.2745e-10, 2.6636e-09, 7.1500e-08,\n",
       "        7.9582e-08, 9.3798e-10, 2.6171e-08, 3.3466e-08, 8.3984e-07, 1.8107e-10,\n",
       "        3.9981e-11, 1.1423e-10, 1.5850e-08, 5.3382e-12, 4.2110e-04, 1.9999e-09,\n",
       "        3.2881e-09, 8.2171e-11, 2.3633e-09, 1.1143e-06, 4.4047e-09, 3.6409e-10,\n",
       "        9.3486e-05, 3.0109e-07, 7.0753e-08, 1.7133e-04, 5.0190e-12, 4.6096e-09,\n",
       "        1.5785e-11, 7.2024e-06], device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "outputs1 = softmax(outputs)\n",
    "outputs1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(outputs1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"meta_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_paths, columns=[\"filepath\"])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(meta_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_predictions = meta_model.predict(meta_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
